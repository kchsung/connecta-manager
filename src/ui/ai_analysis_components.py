"""
ì¸ê³µì§€ëŠ¥ ë¶„ì„ ê´€ë ¨ ì»´í¬ë„ŒíŠ¸ë“¤
"""
import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from typing import Dict, Any, List, Optional
from datetime import datetime, timedelta
import json
import os
import numpy as np
import time
from ..db.database import db_manager
from ..supabase.simple_client import simple_client

def render_ai_analysis_management():
    """ì¸ê³µì§€ëŠ¥ ë¶„ì„ ê´€ë¦¬ ë©”ì¸ ì»´í¬ë„ŒíŠ¸"""
    st.subheader("ğŸ¤– ì¸ê³µì§€ëŠ¥ ë¶„ì„")
    st.markdown("AIë¥¼ í™œìš©í•œ ì¸í”Œë£¨ì–¸ì„œ ë¶„ì„ ë° í‰ê°€ë¥¼ ì œê³µí•©ë‹ˆë‹¤.")
    
    # AI ë¶„ì„ íƒ­ìœ¼ë¡œ ë¶„ë¦¬
    tab1, tab2, tab3 = st.tabs([
        "ğŸš€ ì¸ê³µì§€ëŠ¥ ë¶„ì„ ì‹¤í–‰", 
        "ğŸ“Š ì¸ê³µì§€ëŠ¥ ë¶„ì„ ê²°ê³¼", 
        "ğŸ“ˆ ì¸ê³µì§€ëŠ¥ ë¶„ì„ í†µê³„"
    ])
    
    with tab1:
        render_ai_analysis_execution()
    
    with tab2:
        render_ai_analysis_results()
    
    with tab3:
        render_ai_analysis_statistics()

def render_ai_analysis_execution():
    """AI ë¶„ì„ ì‹¤í–‰ íƒ­"""
    st.subheader("ğŸš€ ì¸ê³µì§€ëŠ¥ ë¶„ì„ ì‹¤í–‰")
    st.markdown("í¬ë¡¤ë§ì´ ì™„ë£Œëœ ì¸í”Œë£¨ì–¸ì„œ ë°ì´í„°ë¥¼ AIë¡œ ë¶„ì„í•©ë‹ˆë‹¤.")
    
    # OpenAI API í‚¤ í™•ì¸
    openai_api_key = os.getenv("OPENAI_API_KEY") or st.secrets.get("OPENAI_API_KEY")
    
    if not openai_api_key:
        st.error("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. secrets.toml ë˜ëŠ” .env íŒŒì¼ì— OPENAI_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        return
    
    # ë¶„ì„ ì¡°ê±´ í‘œì‹œ
    st.info("""
    **ë¶„ì„ ì¡°ê±´:**
    - tb_instagram_crawling í…Œì´ë¸”ì˜ statusê°€ 'COMPLETE'ì¸ ë°ì´í„°ë§Œ ë¶„ì„
    - 1ë‹¬ ì´ë‚´ì— ë¶„ì„ëœ ë°ì´í„°ëŠ” ì¬ë¶„ì„í•˜ì§€ ì•ŠìŒ
    - ìƒˆë¡œìš´ ë°ì´í„°ëŠ” ìƒì„±, ê¸°ì¡´ ë°ì´í„°ëŠ” ì—…ë°ì´íŠ¸
    """)
    
    # ë¶„ì„ ì‹¤í–‰ ë²„íŠ¼
    if st.button("ğŸ¤– AI ë¶„ì„ ì‹œì‘", type="primary", use_container_width=True):
        with st.spinner("AI ë¶„ì„ì„ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤..."):
            try:
                result = execute_ai_analysis()
                if result["success"]:
                    # ìµœì¢… ê²°ê³¼ ìš”ì•½ í‘œì‹œ
                    st.success("ğŸ‰ AI ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
                    
                    # ìƒì„¸ ê²°ê³¼ í‘œì‹œ
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("âœ… ì„±ê³µ", result['analyzed_count'])
                    with col2:
                        st.metric("â­ï¸ ê±´ë„ˆëœ€", result['skipped_count'])
                    with col3:
                        st.metric("âŒ ì‹¤íŒ¨", result.get('failed_count', 0))
                    
                    # ì‹¤íŒ¨í•œ í•­ëª©ì´ ìˆìœ¼ë©´ í‘œì‹œ
                    if result.get('failed_count', 0) > 0:
                        st.warning(f"âš ï¸ {result['failed_count']}ê°œ í•­ëª©ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì‹¤íŒ¨í•œ í•­ëª©ë“¤ì€ ìœ„ì˜ ìƒì„¸ ê²°ê³¼ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                else:
                    st.error(f"AI ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {result['error']}")
            except Exception as e:
                st.error(f"AI ë¶„ì„ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
    
    # ë””ë²„ê¹…ìš©: í¬ë¡¤ë§ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
    if st.button("ğŸ” í¬ë¡¤ë§ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°", use_container_width=True):
        try:
            # ì „ì²´ ë°ì´í„° ê°œìˆ˜ ì¡°íšŒ
            total_count = get_completed_crawling_data_count()
            st.subheader("ğŸ“Š í¬ë¡¤ë§ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°")
            st.write(f"ì´ {total_count:,}ê°œì˜ í¬ë¡¤ë§ ë°ì´í„°ê°€ ìˆìŠµë‹ˆë‹¤.")
            
            if total_count > 0:
                # ì²« ë²ˆì§¸ ë°°ì¹˜ ë°ì´í„° ì¡°íšŒ (ìµœëŒ€ 5ê°œ)
                crawling_data = get_completed_crawling_data(limit=5, offset=0)
                
                if crawling_data:
                    # ì²« ë²ˆì§¸ ë°ì´í„° êµ¬ì¡° í™•ì¸
                    first_data = crawling_data[0]
                    st.write("**ì²« ë²ˆì§¸ ë°ì´í„° êµ¬ì¡°:**")
                    st.json(first_data)
                    
                    # AI ì…ë ¥ ë°ì´í„° êµ¬ì„± ì˜ˆì‹œ
                    ai_input_example = {
                        "id": first_data.get("id", ""),
                        "description": first_data.get("description", ""),
                        "posts": first_data.get("posts", "")[:500] + "..." if len(first_data.get("posts", "")) > 500 else first_data.get("posts", "")
                    }
                    st.write("**AI ë¶„ì„ìš© ì…ë ¥ ë°ì´í„° ì˜ˆì‹œ:**")
                    st.json(ai_input_example)
                    
                    # posts í•„ë“œ í™•ì¸
                    posts_content = first_data.get("posts", "")
                    if posts_content:
                        st.write("**posts í•„ë“œ ë‚´ìš© (ì²˜ìŒ 500ì):**")
                        st.text(posts_content[:500])
                    else:
                        st.warning("posts í•„ë“œê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                    
                    # ë°°ì¹˜ ì²˜ë¦¬ ì •ë³´ í‘œì‹œ
                    st.info(f"""
                    **ë°°ì¹˜ ì²˜ë¦¬ ì •ë³´:**
                    - ì´ ë°ì´í„°: {total_count:,}ê°œ
                    - ë°°ì¹˜ í¬ê¸°: 100ê°œì”©
                    - ì˜ˆìƒ ë°°ì¹˜ ìˆ˜: {(total_count + 99) // 100}ê°œ
                    - ì²˜ë¦¬ ì‹œê°„ ì˜ˆìƒ: ì•½ {total_count * 2 // 60}ë¶„ (ê°œë‹¹ 2ì´ˆ ê¸°ì¤€)
                    """)
                else:
                    st.warning("ë°ì´í„° ì¡°íšŒì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            else:
                st.warning("í¬ë¡¤ë§ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            st.error(f"ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° ì¤‘ ì˜¤ë¥˜: {str(e)}")

def execute_ai_analysis():
    """AI ë¶„ì„ ì‹¤í–‰ í•¨ìˆ˜ (ë°°ì¹˜ ì²˜ë¦¬) - ì•ˆì •í™” ë²„ì „"""
    try:
        # Supabase í´ë¼ì´ì–¸íŠ¸ 1íšŒ ìƒì„±/ì¬ì‚¬ìš©
        client = simple_client.get_client()
        if not client:
            return {"success": False, "error": "Supabase í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì‹¤íŒ¨"}

        # 1. ì „ì²´ ë°ì´í„° ê°œìˆ˜ (COMPLETE ìƒíƒœë§Œ)
        total_count = get_completed_crawling_data_count(client)
        if total_count == 0:
            return {"success": False, "error": "ë¶„ì„í•  í¬ë¡¤ë§ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}

        # ë””ë²„ê¹…: ì „ì²´ ë°ì´í„° ê°œìˆ˜ë„ í™•ì¸
        try:
            total_all_count = client.table("tb_instagram_crawling").select("id", count="exact").execute()
            st.info(f"ğŸ“Š ë°ì´í„° í˜„í™©: ì „ì²´ {total_all_count.count:,}ê°œ ì¤‘ ì™„ë£Œëœ í¬ë¡¤ë§ ë°ì´í„° {total_count:,}ê°œ (status='COMPLETE')")
        except:
            st.info(f"ì´ {total_count:,}ê°œì˜ ì™„ë£Œëœ í¬ë¡¤ë§ ë°ì´í„°(status='COMPLETE')ê°€ ìˆìŠµë‹ˆë‹¤.")
        
        st.info("ë°°ì¹˜ ë‹¨ìœ„ë¡œ AI ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤.")

        batch_size = 50
        total_batches = (total_count + batch_size - 1) // batch_size

        analyzed_count = 0
        skipped_count = 0
        failed_count = 0
        processed_count = 0
        failed_items = []

        overall_progress_bar = st.progress(0)
        overall_status_text = st.empty()
        result_container = st.empty()

        UI_UPDATE_EVERY = 50  # ê°±ì‹  ì£¼ê¸° ì¤„ì´ê¸°

        for batch_num in range(total_batches):
            offset = batch_num * batch_size
            batch_data = get_completed_crawling_data(client, limit=batch_size, offset=offset)
            if not batch_data:
                break

            # ë°°ì¹˜ ì§„í–‰ UI (ê°„ì†Œí™”)
            batch_progress_bar = st.progress(0)
            batch_status_text = st.empty()

            for index, data in enumerate(batch_data):
                current_id = data.get('id', 'unknown')
                try:
                    # ì „ì²´/ë°°ì¹˜ ì§„í–‰ë¥ 
                    overall_progress = (processed_count + index + 1) / total_count
                    overall_progress_bar.progress(overall_progress)
                    overall_status_text.text(
                        f"ì „ì²´ ì§„í–‰: {processed_count + index + 1:,}/{total_count:,} (ë°°ì¹˜ {batch_num + 1}/{total_batches})"
                    )
                    batch_progress = (index + 1) / len(batch_data)
                    batch_progress_bar.progress(batch_progress)
                    batch_status_text.text(f"ë°°ì¹˜ {batch_num + 1} ì§„í–‰: {index + 1}/{len(batch_data)} - {current_id}")

                    # 1) ìµœê·¼ ë¶„ì„ ì—¬ë¶€ ë¨¼ì € (DB/API í˜¸ì¶œ ì ˆì•½)
                    if is_recently_analyzed_by_id(client, data["id"]):
                        skipped_count += 1
                        continue

                    # 2) ì…ë ¥ êµ¬ì„± (postsëŠ” ìë¥´ì§€ ì•ŠìŒ)
                    posts_content = data.get("posts", "") or ""
                    if not posts_content:
                        skipped_count += 1
                        continue

                    ai_input_data = {
                        "id": data.get("id", ""),
                        "description": data.get("description", "") or "",
                        "posts": posts_content
                    }

                    # 3) AI ë¶„ì„
                    analysis_result = perform_ai_analysis(ai_input_data)
                    if not analysis_result:
                        failed_items.append({"id": current_id, "error": "AI ë¶„ì„ ì‹¤íŒ¨"})
                        failed_count += 1
                        continue

                    # 4) ë³€í™˜
                    transformed_result = transform_to_db_format(ai_input_data, analysis_result, data["id"])
                    if not transformed_result:
                        failed_items.append({"id": current_id, "error": "ë°ì´í„° ë³€í™˜ ì‹¤íŒ¨"})
                        failed_count += 1
                        continue

                    # 5) ì €ì¥
                    try:
                        save_ai_analysis_result(client, data, transformed_result, data["id"])
                        analyzed_count += 1
                    except Exception as se:
                        failed_items.append({"id": current_id, "error": f"ì €ì¥ ì‹¤íŒ¨: {str(se)}"})
                        failed_count += 1
                        continue

                    # UI ì—…ë°ì´íŠ¸(í¬ì†Œ)
                    if ((index + 1) % UI_UPDATE_EVERY == 0) or (index == len(batch_data) - 1):
                        with result_container.container():
                            st.markdown("### ğŸ“Š ì‹¤ì‹œê°„ ì²˜ë¦¬ ê²°ê³¼")
                            c1, c2, c3, c4 = st.columns(4)
                            c1.metric("âœ… ì„±ê³µ", analyzed_count)
                            c2.metric("â­ï¸ ê±´ë„ˆëœ€", skipped_count)
                            c3.metric("âŒ ì‹¤íŒ¨", failed_count)
                            c4.metric("ğŸ“Š ì´ ì²˜ë¦¬", processed_count + index + 1)

                except Exception as e:
                    failed_items.append({"id": current_id, "error": f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}"})
                    failed_count += 1
                    continue

            processed_count += len(batch_data)
            batch_progress_bar.empty()
            batch_status_text.empty()

            # ë°°ì¹˜ ê°„ íœ´ì‹ ìµœì†Œí™”(ë˜ëŠ” ì œê±°)
            if batch_num < total_batches - 1:
                time.sleep(0.1)

        overall_progress_bar.progress(1.0)
        overall_status_text.text("ë¶„ì„ ì™„ë£Œ!")

        with result_container.container():
            st.markdown("### ğŸ‰ AI ë¶„ì„ ìµœì¢… ê²°ê³¼")
            c1, c2, c3, c4 = st.columns(4)
            c1.metric("âœ… ì„±ê³µ", analyzed_count, delta=f"{(analyzed_count/total_count*100):.1f}%")
            c2.metric("â­ï¸ ê±´ë„ˆëœ€", skipped_count, delta=f"{(skipped_count/total_count*100):.1f}%")
            c3.metric("âŒ ì‹¤íŒ¨", failed_count, delta=f"{(failed_count/total_count*100):.1f}%")
            c4.metric("ğŸ“Š ì´ ì²˜ë¦¬", total_count, delta="100%")

            if failed_items:
                st.markdown("### âŒ ì‹¤íŒ¨í•œ í•­ëª©ë“¤")
                with st.expander(f"ì‹¤íŒ¨í•œ {len(failed_items)}ê°œ í•­ëª© ìƒì„¸ë³´ê¸°"):
                    for item in failed_items:
                        st.error(f"**ID: {item['id']}** - {item['error']}")

        return {
            "success": True,
            "analyzed_count": analyzed_count,
            "skipped_count": skipped_count,
            "failed_count": failed_count,
            "total_count": total_count,
            "failed_items": failed_items
        }

    except Exception as e:
        return {"success": False, "error": str(e)}

def get_completed_crawling_data(client, limit=1000, offset=0):
    """í¬ë¡¤ë§ ì™„ë£Œëœ ë°ì´í„° ì¡°íšŒ (í˜ì´ì§•) - ì¬ì‹œë„ í¬í•¨"""
    max_retries = 3
    retry_delay = 1
    for attempt in range(max_retries):
        try:
            if not client:
                return []
            response = client.table("tb_instagram_crawling").select("*")\
                .eq("status", "COMPLETE").range(offset, offset + limit - 1).execute()
            return response.data if response.data else []
        except Exception as e:
            error_msg = str(e)
            if "Server disconnected" in error_msg or "connection" in error_msg.lower():
                if attempt < max_retries - 1:
                    st.warning(f"ì„œë²„ ì—°ê²° ì˜¤ë¥˜. {retry_delay}s í›„ ì¬ì‹œë„... ({attempt + 1}/{max_retries})")
                    time.sleep(retry_delay); retry_delay *= 2; continue
                else:
                    st.error(f"í¬ë¡¤ë§ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨(ì¬ì‹œë„ ì´ˆê³¼): {error_msg}")
                    return []
            else:
                st.error(f"í¬ë¡¤ë§ ë°ì´í„° ì¡°íšŒ ì˜¤ë¥˜: {error_msg}")
                return []
    return []

def get_completed_crawling_data_count(client):
    """í¬ë¡¤ë§ ì™„ë£Œëœ ë°ì´í„° ì´ ê°œìˆ˜"""
    max_retries = 3
    retry_delay = 1
    for attempt in range(max_retries):
        try:
            if not client:
                return 0
            response = client.table("tb_instagram_crawling").select("id", count="exact")\
                .eq("status", "COMPLETE").execute()
            return response.count if response.count else 0
        except Exception as e:
            error_msg = str(e)
            if "Server disconnected" in error_msg or "connection" in error_msg.lower():
                if attempt < max_retries - 1:
                    st.warning(f"ì„œë²„ ì—°ê²° ì˜¤ë¥˜. {retry_delay}s í›„ ì¬ì‹œë„... ({attempt + 1}/{max_retries})")
                    time.sleep(retry_delay); retry_delay *= 2; continue
                else:
                    st.error(f"ê°œìˆ˜ ì¡°íšŒ ì‹¤íŒ¨(ì¬ì‹œë„ ì´ˆê³¼): {error_msg}")
                    return 0
            else:
                st.error(f"ê°œìˆ˜ ì¡°íšŒ ì˜¤ë¥˜: {error_msg}")
                return 0
    return 0

def is_recently_analyzed(influencer_id, platform):
    """ìµœê·¼ ë¶„ì„ ì—¬ë¶€ í™•ì¸ (1ë‹¬ ì´ë‚´) - ê¸°ì¡´ í•¨ìˆ˜ (í˜¸í™˜ì„± ìœ ì§€)"""
    try:
        one_month_ago = datetime.now() - timedelta(days=30)
        
        client = simple_client.get_client()
        if not client:
            return False
        
        response = client.table("ai_influencer_analyses").select("analyzed_at").eq("influencer_id", influencer_id).eq("platform", platform).gte("analyzed_at", one_month_ago.isoformat()).execute()
        
        return len(response.data) > 0 if response.data else False
    except Exception as e:
        st.error(f"ìµœê·¼ ë¶„ì„ ì—¬ë¶€ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return False

def is_recently_analyzed_by_id(client, crawling_id):
    """í¬ë¡¤ë§ ID ìµœê·¼ ë¶„ì„ ì—¬ë¶€(30ì¼)"""
    max_retries = 3
    retry_delay = 1
    for attempt in range(max_retries):
        try:
            if not client:
                return False
            one_month_ago = datetime.now() - timedelta(days=30)
            response = client.table("ai_influencer_analyses").select("analyzed_at")\
                .eq("influencer_id", crawling_id).eq("platform", "instagram")\
                .gte("analyzed_at", one_month_ago.isoformat()).execute()
            return bool(response.data)
        except Exception as e:
            error_msg = str(e)
            if "Server disconnected" in error_msg or "connection" in error_msg.lower():
                if attempt < max_retries - 1:
                    st.warning(f"ì„œë²„ ì—°ê²° ì˜¤ë¥˜. {retry_delay}s í›„ ì¬ì‹œë„... ({attempt + 1}/{max_retries})")
                    time.sleep(retry_delay); retry_delay *= 2; continue
                else:
                    st.error(f"ìµœê·¼ ë¶„ì„ ì—¬ë¶€ í™•ì¸ ì‹¤íŒ¨(ì¬ì‹œë„ ì´ˆê³¼): {error_msg}")
                    return False
            else:
                st.error(f"ìµœê·¼ ë¶„ì„ ì—¬ë¶€ í™•ì¸ ì˜¤ë¥˜: {error_msg}")
                return False
    return False

def perform_ai_analysis(data):
    """AI ë¶„ì„ ìˆ˜í–‰ - ìš”ì²­ë³„ íƒ€ì„ì•„ì›ƒ + íŠ¼íŠ¼í•œ ì¬ì‹œë„"""
    from openai import OpenAI
    import random, time

    max_retries = 5
    timeout_seconds = 200  # ìš”ì²­ë³„ íƒ€ì„ì•„ì›ƒ

    api_key = os.getenv("OPENAI_API_KEY") or st.secrets.get("OPENAI_API_KEY")
    if not api_key:
        st.error("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return None

    client = OpenAI(api_key=api_key)

    # ëª¨ë¸ ëª…ì‹œ í•„ìˆ˜ (secretsì—ì„œ ì˜¤ë²„ë¼ì´ë“œ ê°€ëŠ¥)
    model = st.secrets.get("OPENAI_MODEL", "gpt-5-mini")
    prompt_id = st.secrets.get("OPENAI_PROMPT_ID", "pmpt_68f36e44eab08196b4e75067a3074b7b0c099d8443a9dd49")
    prompt_version = st.secrets.get("OPENAI_PROMPT_VERSION", "4")

    input_data = json.dumps(data, ensure_ascii=False)

    for attempt in range(1, max_retries + 1):
        try:
            resp = client.responses.create(
                model=model,
                prompt={"id": prompt_id, "version": prompt_version},
                input=input_data,
                reasoning={"summary": "auto"},
                store=True,
                include=["reasoning.encrypted_content", "web_search_call.action.sources"],
                timeout=timeout_seconds,  # ìš”ì²­ë³„ timeout
            )
            return parse_ai_response(resp)

        except Exception as e:
            msg = str(e).lower()

            # Retry-After í—¤ë”ê°€ ìˆìœ¼ë©´ ì¡´ì¤‘
            retry_after = 0
            if hasattr(e, "response") and getattr(e.response, "headers", None):
                ra = e.response.headers.get("retry-after")
                if ra:
                    try:
                        retry_after = int(ra)
                    except:
                        retry_after = 0

            # ë ˆì´íŠ¸ë¦¬ë°‹/ì¿¼í„°
            if "rate limit" in msg or "quota" in msg or "too many requests" in msg or "429" in msg:
                wait = max(retry_after, min(40, 2 ** attempt + random.uniform(0, 1)))
                st.warning(f"[OpenAI] Rate limit: {attempt}/{max_retries} ì¬ì‹œë„, {wait:.1f}s ëŒ€ê¸°")
                time.sleep(wait)
                continue

            # íƒ€ì„ì•„ì›ƒ/ê²Œì´íŠ¸ì›¨ì´
            if "timeout" in msg or "timed out" in msg or "504" in msg or "gateway" in msg:
                wait = min(30, 2 ** attempt)
                st.warning(f"[OpenAI] Timeout: {attempt}/{max_retries} ì¬ì‹œë„, {wait}s ëŒ€ê¸°")
                time.sleep(wait)
                continue

            # ê·¸ ì™¸ ì—ëŸ¬ëŠ” ì¤‘ë‹¨(ë¡œê·¸ë§Œ)
            st.error(f"AI ë¶„ì„ ìˆ˜í–‰ ì¤‘ ì˜¤ë¥˜(ì¤‘ë‹¨): {e}")
            return None

    st.error("OpenAI API ì¬ì‹œë„ í•œë„ ì´ˆê³¼")
    return None

def parse_ai_response(response):
    """Responses API í‘œì¤€ íŒŒì„œ: output_text ìš°ì„ , fallbackë¡œ content[*].text, ì½”ë“œíœìŠ¤ JSON ì¶”ì¶œ"""
    try:
        text = None

        if getattr(response, "output_text", None):
            text = response.output_text
        elif getattr(response, "output", None):
            chunks = []
            for block in (response.output or []):
                for c in getattr(block, "content", []) or []:
                    if hasattr(c, "text") and c.text:
                        chunks.append(c.text)
            text = "\n".join(chunks) if chunks else None

        if not text:
            st.error("ì‘ë‹µì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return None

        import re, json as _json
        # ```json ... ``` ìš°ì„ 
        m = re.search(r"```json\s*(\{.*?\}|\[.*?\])\s*```", text, flags=re.S)
        if m:
            text = m.group(1)

        return _json.loads(text)

    except Exception as e:
        st.error(f"AI ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜: {e}")
        return None

def transform_to_db_format(ai_input_data, ai_result, crawling_id):
    """AI ë¶„ì„ ê²°ê³¼ë¥¼ ai_influencer_analyses í…Œì´ë¸” êµ¬ì¡°ì— ë§ê²Œ ë³€í™˜"""
    try:
        # ê¸°ë³¸ ë°ì´í„° ì¶”ì¶œ (AI ë¶„ì„ ê²°ê³¼ì—ì„œ ì¶”ì¶œ)
        # ai_input_dataëŠ” {"id": "", "description": "", "posts": ""} í˜•íƒœ
        influencer_id = crawling_id  # influencer_idëŠ” ì´ì œ VARCHAR íƒ€ì…ì´ë¯€ë¡œ crawling_id ì‚¬ìš©
        platform = "instagram"  # tb_instagram_crawlingì€ ëª¨ë‘ instagram ë°ì´í„°
        
        # AI ë¶„ì„ ê²°ê³¼ì—ì„œ ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ (AIê°€ ë¶„ì„í•œ ê²°ê³¼ ì‚¬ìš©)
        name = ai_result.get("name", "")
        alias = ai_input_data.get("id", "")  # idë¥¼ aliasë¡œ ì‚¬ìš©
        followers = ai_result.get("followers", 0)
        followings = ai_result.get("followings", 0)
        posts_count = ai_result.get("posts_count", 0)
        
        # AI ë¶„ì„ ê²°ê³¼ì—ì„œ ë°ì´í„° ì¶”ì¶œ
        category = ai_result.get("category", "ê¸°íƒ€")
        tags = ai_result.get("tags", [])
        follow_network_analysis = ai_result.get("follow_network_analysis", {})
        comment_authenticity_analysis = ai_result.get("comment_authenticity_analysis", {})
        content_analysis = ai_result.get("content_analysis", {})
        evaluation = ai_result.get("evaluation", {})
        insights = ai_result.get("insights", {})
        summary = ai_result.get("summary", "")
        recommendation = ai_result.get("recommendation", "ë³´í†µ")
        notes = ai_result.get("notes", {})
        
        # ë””ë²„ê¹…: AI ì‘ë‹µ êµ¬ì¡° í™•ì¸
        st.write("ğŸ” AI ì‘ë‹µ êµ¬ì¡° í™•ì¸:")
        st.write(f"- name: {name}")
        st.write(f"- category: {category}")
        st.write(f"- tags: {tags}")
        st.write(f"- recommendation: {recommendation}")
        st.write(f"- evaluation keys: {list(evaluation.keys()) if evaluation else 'None'}")
        st.write(f"- content_analysis keys: {list(content_analysis.keys()) if content_analysis else 'None'}")
        
        # ì¶”ì²œë„ ìœ íš¨ì„± ê²€ì¦ ë° ë³€í™˜
        valid_recommendations = ["ë§¤ìš° ì¶”ì²œ", "ì¶”ì²œ", "ë³´í†µ", "ë¹„ì¶”ì²œ", "ë§¤ìš° ë¹„ì¶”ì²œ", "ì¡°ê±´ë¶€"]
        if recommendation not in valid_recommendations:
            # "ì¡°ê±´ë¶€" ì¶”ì²œë„ëŠ” "ë³´í†µ"ìœ¼ë¡œ ë³€í™˜
            if recommendation == "ì¡°ê±´ë¶€":
                recommendation = "ë³´í†µ"
            else:
                recommendation = "ë³´í†µ"
        
        # ì ìˆ˜ ìœ íš¨ì„± ê²€ì¦ (0-10 ë²”ìœ„)
        def validate_score(score, default=0):
            try:
                score_val = float(score) if score is not None else default
                return max(0, min(10, score_val))
            except (ValueError, TypeError):
                return default
        
        # evaluation ì ìˆ˜ë“¤ ê²€ì¦
        if isinstance(evaluation, dict):
            evaluation["engagement"] = validate_score(evaluation.get("engagement", 0))
            evaluation["activity"] = validate_score(evaluation.get("activity", 0))
            evaluation["communication"] = validate_score(evaluation.get("communication", 0))
            evaluation["growth_potential"] = validate_score(evaluation.get("growth_potential", 0))
            evaluation["overall_score"] = validate_score(evaluation.get("overall_score", 0))
        
        # inference_confidence ê²€ì¦ (0-1 ë²”ìœ„)
        if isinstance(content_analysis, dict):
            confidence = content_analysis.get("inference_confidence", 0.5)
            try:
                confidence_val = float(confidence) if confidence is not None else 0.5
                content_analysis["inference_confidence"] = max(0, min(1, confidence_val))
            except (ValueError, TypeError):
                content_analysis["inference_confidence"] = 0.5
        
        # notesì— í¬ë¡¤ë§ ID ì¶”ê°€ (ë‚˜ì¤‘ì— save_ai_analysis_resultì—ì„œ ì„¤ì •ë¨)
        if not isinstance(notes, dict):
            notes = {}
        
        # ìµœì¢… ë°ì´í„° êµ¬ì¡° ìƒì„±
        db_data = {
            "influencer_id": influencer_id,
            "platform": platform,
            "name": name,
            "alias": alias,
            "followers": followers,
            "followings": followings,
            "posts_count": posts_count,
            "category": category,
            "tags": tags,
            "follow_network_analysis": follow_network_analysis,
            "comment_authenticity_analysis": comment_authenticity_analysis,
            "content_analysis": content_analysis,
            "evaluation": evaluation,
            "insights": insights,
            "summary": summary,
            "recommendation": recommendation,
            "notes": notes,
            "source": "ai_auto",
            "analyzed_at": datetime.now().isoformat(),
            "analyzed_on": datetime.now().date().isoformat()
        }
        
        # ì ìˆ˜ ê´€ë ¨ ì»¬ëŸ¼ë“¤ì€ ëª¨ë‘ generated columnì´ë¯€ë¡œ ì§ì ‘ ì„¤ì •í•˜ì§€ ì•ŠìŒ
        # evaluation ì ìˆ˜ë“¤ì€ evaluation JSON í•„ë“œì— ì €ì¥ë˜ê³ , 
        # DBì—ì„œ generated columnìœ¼ë¡œ ìë™ ê³„ì‚°ë¨
        # if isinstance(evaluation, dict):
        #     db_data["engagement_score"] = evaluation.get("engagement")
        #     db_data["activity_score"] = evaluation.get("activity")
        #     db_data["communication_score"] = evaluation.get("communication")
        #     db_data["growth_potential_score"] = evaluation.get("growth_potential")
        #     db_data["overall_score"] = evaluation.get("overall_score")

        # inference_confidenceë„ generated columnì´ë¯€ë¡œ ì§ì ‘ ì„¤ì •í•˜ì§€ ì•ŠìŒ
        # if isinstance(content_analysis, dict):
        #     db_data["inference_confidence"] = content_analysis.get("inference_confidence")
        
        return db_data
        
    except Exception as e:
        st.error(f"ë°ì´í„° ë³€í™˜ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None

def save_ai_analysis_result(client, crawling_data, analysis_result, crawling_id):
    """AI ë¶„ì„ ê²°ê³¼ ì €ì¥ - client ì£¼ì… ë²„ì „"""
    max_retries = 3
    retry_delay = 1
    for attempt in range(max_retries):
        try:
            if not client:
                raise Exception("Supabase í´ë¼ì´ì–¸íŠ¸ ì—†ìŒ")

            # ì¶”ì ìš© crawling_id ì£¼ì…
            if "notes" in analysis_result and isinstance(analysis_result["notes"], dict):
                analysis_result["notes"]["crawling_id"] = crawling_id

            existing_response = client.table("ai_influencer_analyses").select("id")\
                .eq("influencer_id", crawling_id).eq("platform", "instagram").execute()

            if existing_response.data:
                client.table("ai_influencer_analyses").update(analysis_result)\
                    .eq("id", existing_response.data[0]["id"]).execute()
            else:
                client.table("ai_influencer_analyses").insert(analysis_result).execute()
            return

        except Exception as e:
            error_msg = str(e)
            if "Server disconnected" in error_msg or "connection" in error_msg.lower():
                if attempt < max_retries - 1:
                    st.warning(f"ì„œë²„ ì—°ê²° ì˜¤ë¥˜. {retry_delay}s í›„ ì¬ì‹œë„... ({attempt + 1}/{max_retries})")
                    time.sleep(retry_delay); retry_delay *= 2; continue
                else:
                    st.error(f"AI ë¶„ì„ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨(ì¬ì‹œë„ ì´ˆê³¼): {error_msg}")
                    raise
            else:
                st.error(f"AI ë¶„ì„ ê²°ê³¼ ì €ì¥ ì˜¤ë¥˜: {error_msg}")
                raise

def render_ai_analysis_results():
    """AI ë¶„ì„ ê²°ê³¼ íƒ­"""
    st.subheader("ğŸ“Š ì¸ê³µì§€ëŠ¥ ë¶„ì„ ê²°ê³¼")
    st.markdown("AI ë¶„ì„ ê²°ê³¼ë¥¼ ì¡°íšŒí•˜ê³  í•„í„°ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    # ê²€ìƒ‰ ë° í•„í„°ë§
    col1, col2, col3 = st.columns([2, 1, 1])
    
    with col1:
        search_term = st.text_input("ğŸ” ê²€ìƒ‰ (ì´ë¦„, íƒœê·¸, influencer_id)", placeholder="ì¸í”Œë£¨ì–¸ì„œ ì´ë¦„, íƒœê·¸, ë˜ëŠ” influencer_idë¥¼ ì…ë ¥í•˜ì„¸ìš”")
    
    with col2:
        category_filter = st.selectbox("ğŸ“‚ ì¹´í…Œê³ ë¦¬", ["ì „ì²´"] + get_categories())
    
    with col3:
        recommendation_filter = st.selectbox("â­ ì¶”ì²œë„", ["ì „ì²´", "ë§¤ìš° ì¶”ì²œ", "ì¶”ì²œ", "ë³´í†µ", "ë¹„ì¶”ì²œ", "ë§¤ìš° ë¹„ì¶”ì²œ"])
    
    # ê²€ìƒ‰ ì¡°ê±´ì´ ë³€ê²½ë˜ë©´ í˜ì´ì§€ ì´ˆê¸°í™”
    current_filters = f"{search_term}_{category_filter}_{recommendation_filter}"
    if 'last_filters' not in st.session_state or st.session_state.last_filters != current_filters:
        st.session_state.analysis_page = 1
        st.session_state.last_filters = current_filters
    
    # ë¶„ì„ ê²°ê³¼ ì¡°íšŒ
    try:
        # í˜ì´ì§• ì„¤ì •
        page_size = 50  # í•œ í˜ì´ì§€ë‹¹ í‘œì‹œí•  í•­ëª© ìˆ˜
        page = st.session_state.get('analysis_page', 1)
        
        # ì „ì²´ ê°œìˆ˜ ì¡°íšŒ
        total_count = get_ai_analysis_data_count(search_term, category_filter, recommendation_filter)
        
        if total_count == 0:
            st.warning("ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # í˜ì´ì§• ê³„ì‚°
        total_pages = (total_count + page_size - 1) // page_size
        offset = (page - 1) * page_size
        
        # í˜„ì¬ í˜ì´ì§€ ë°ì´í„° ì¡°íšŒ
        analysis_data = get_ai_analysis_data(search_term, category_filter, recommendation_filter, page_size, offset)
        
        if not analysis_data:
            st.warning("ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # í™”ë©´ì„ ì¢Œìš°ë¡œ ë¶„í• 
        left_col, right_col = st.columns([1, 2])
        
        with left_col:
            st.markdown("### ğŸ“‹ ê²€ìƒ‰ ê²°ê³¼")
            st.markdown(f"ì´ {total_count:,}ê°œì˜ ê²°ê³¼ (í˜ì´ì§€ {page}/{total_pages})")
            
            # í˜ì´ì§• ì»¨íŠ¸ë¡¤
            if total_pages > 1:
                col1, col2, col3, col4, col5 = st.columns(5)
                
                with col1:
                    if st.button("â®ï¸ ì²« í˜ì´ì§€", disabled=(page == 1)):
                        st.session_state.analysis_page = 1
                        st.rerun()
                
                with col2:
                    if st.button("â¬…ï¸ ì´ì „", disabled=(page == 1)):
                        st.session_state.analysis_page = page - 1
                        st.rerun()
                
                with col3:
                    st.write(f"**{page}**")
                
                with col4:
                    if st.button("ë‹¤ìŒ â¡ï¸", disabled=(page == total_pages)):
                        st.session_state.analysis_page = page + 1
                        st.rerun()
                
                with col5:
                    if st.button("ë§ˆì§€ë§‰ â­ï¸", disabled=(page == total_pages)):
                        st.session_state.analysis_page = total_pages
                        st.rerun()
            
            # ì¢Œì¸¡: ê²€ìƒ‰ ë¦¬ìŠ¤íŠ¸ (ì´ë¦„, ì•„ì´ë””, í”Œë«í¼ëª…ë§Œ í‘œì‹œ)
            selected_analysis = None
            for i, analysis in enumerate(analysis_data):
                # ê° í•­ëª©ì„ í´ë¦­ ê°€ëŠ¥í•œ ë²„íŠ¼ìœ¼ë¡œ í‘œì‹œ
                if st.button(
                    f"ğŸ“Š {analysis['name']}\n"
                    f"ğŸ†” {analysis['influencer_id']}\n"
                    f"ğŸ“± {analysis['platform']}",
                    key=f"select_{analysis['id']}_{page}",  # í˜ì´ì§€ë³„ë¡œ ê³ ìœ  í‚¤ ìƒì„±
                    use_container_width=True
                ):
                    selected_analysis = analysis
                    st.session_state.selected_analysis = analysis
        
        with right_col:
            st.markdown("### ğŸ“Š ìƒì„¸ ì •ë³´")
            
            # ì„¸ì…˜ ìƒíƒœì—ì„œ ì„ íƒëœ ë¶„ì„ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
            if 'selected_analysis' in st.session_state:
                selected_analysis = st.session_state.selected_analysis
            
            if selected_analysis:
                try:
                    show_detailed_analysis_improved(selected_analysis)
                except Exception as e:
                    st.error(f"ìƒì„¸ ì •ë³´ í‘œì‹œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
                    st.info("ë‹¤ë¥¸ ì¸í”Œë£¨ì–¸ì„œë¥¼ ì„ íƒí•´ë³´ì„¸ìš”.")
            else:
                st.info("ì¢Œì¸¡ì—ì„œ ì¸í”Œë£¨ì–¸ì„œë¥¼ ì„ íƒí•˜ë©´ ìƒì„¸ ì •ë³´ê°€ í‘œì‹œë©ë‹ˆë‹¤.")
    
    except Exception as e:
        st.error(f"ë¶„ì„ ê²°ê³¼ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")

def get_ai_analysis_data(search_term="", category_filter="ì „ì²´", recommendation_filter="ì „ì²´", limit=1000, offset=0):
    """AI ë¶„ì„ ë°ì´í„° ì¡°íšŒ (í˜ì´ì§• ì§€ì›) - ì¬ì‹œë„ ë¡œì§ í¬í•¨"""
    max_retries = 3
    retry_delay = 1
    
    for attempt in range(max_retries):
        try:
            client = simple_client.get_client()
            if not client:
                return []
            
            query = client.table("ai_influencer_analyses").select("*")
            
            # ê²€ìƒ‰ ì¡°ê±´
            if search_term:
                # ì´ë¦„, íƒœê·¸, influencer_idì—ì„œ ê²€ìƒ‰
                query = query.or_(f"name.ilike.%{search_term}%,tags.cs.{{{search_term}}},influencer_id.ilike.%{search_term}%")
            
            # ì¹´í…Œê³ ë¦¬ í•„í„°
            if category_filter != "ì „ì²´":
                query = query.eq("category", category_filter)
            
            # ì¶”ì²œë„ í•„í„°
            if recommendation_filter != "ì „ì²´":
                query = query.eq("recommendation", recommendation_filter)
            
            response = query.order("analyzed_at", desc=True).range(offset, offset + limit - 1).execute()
            return response.data if response.data else []
            
        except Exception as e:
            error_msg = str(e)
            if "Server disconnected" in error_msg or "connection" in error_msg.lower():
                if attempt < max_retries - 1:
                    st.warning(f"ì„œë²„ ì—°ê²° ì˜¤ë¥˜. {retry_delay}s í›„ ì¬ì‹œë„... ({attempt + 1}/{max_retries})")
                    time.sleep(retry_delay)
                    retry_delay *= 2
                    continue
                else:
                    st.error(f"ë¶„ì„ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨(ì¬ì‹œë„ ì´ˆê³¼): {error_msg}")
                    return []
            else:
                st.error(f"ë¶„ì„ ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {error_msg}")
                return []
    
    return []

def get_ai_analysis_data_count(search_term="", category_filter="ì „ì²´", recommendation_filter="ì „ì²´"):
    """AI ë¶„ì„ ë°ì´í„° ì´ ê°œìˆ˜ ì¡°íšŒ - ì¬ì‹œë„ ë¡œì§ í¬í•¨"""
    max_retries = 3
    retry_delay = 1
    
    for attempt in range(max_retries):
        try:
            client = simple_client.get_client()
            if not client:
                return 0
            
            query = client.table("ai_influencer_analyses").select("id", count="exact")
            
            # ê²€ìƒ‰ ì¡°ê±´
            if search_term:
                # ì´ë¦„, íƒœê·¸, influencer_idì—ì„œ ê²€ìƒ‰
                query = query.or_(f"name.ilike.%{search_term}%,tags.cs.{{{search_term}}},influencer_id.ilike.%{search_term}%")
            
            # ì¹´í…Œê³ ë¦¬ í•„í„°
            if category_filter != "ì „ì²´":
                query = query.eq("category", category_filter)
            
            # ì¶”ì²œë„ í•„í„°
            if recommendation_filter != "ì „ì²´":
                query = query.eq("recommendation", recommendation_filter)
            
            response = query.execute()
            return response.count if response.count else 0
            
        except Exception as e:
            error_msg = str(e)
            if "Server disconnected" in error_msg or "connection" in error_msg.lower():
                if attempt < max_retries - 1:
                    st.warning(f"ì„œë²„ ì—°ê²° ì˜¤ë¥˜. {retry_delay}s í›„ ì¬ì‹œë„... ({attempt + 1}/{max_retries})")
                    time.sleep(retry_delay)
                    retry_delay *= 2
                    continue
                else:
                    st.error(f"ë¶„ì„ ë°ì´í„° ê°œìˆ˜ ì¡°íšŒ ì‹¤íŒ¨(ì¬ì‹œë„ ì´ˆê³¼): {error_msg}")
                    return 0
            else:
                st.error(f"ë¶„ì„ ë°ì´í„° ê°œìˆ˜ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {error_msg}")
                return 0
    
    return 0

def get_categories():
    """ì¹´í…Œê³ ë¦¬ ëª©ë¡ ì¡°íšŒ - ì¬ì‹œë„ ë¡œì§ í¬í•¨"""
    max_retries = 3
    retry_delay = 1
    
    for attempt in range(max_retries):
        try:
            client = simple_client.get_client()
            if not client:
                return []
            
            response = client.table("ai_influencer_analyses").select("category").execute()
            categories = list(set([item["category"] for item in response.data if item.get("category")]))
            return sorted(categories)
            
        except Exception as e:
            error_msg = str(e)
            if "Server disconnected" in error_msg or "connection" in error_msg.lower():
                if attempt < max_retries - 1:
                    time.sleep(retry_delay)
                    retry_delay *= 2
                    continue
                else:
                    return []
            else:
                return []
    
    return []

def display_analysis_section(data):
    """ë¶„ì„ ì„¹ì…˜ ë°ì´í„°ë¥¼ ì½ê¸° ì‰¬ìš´ í˜•íƒœë¡œ í‘œì‹œ"""
    if not data:
        st.info("ë¶„ì„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    if isinstance(data, dict):
        for key, value in data.items():
            korean_key = get_korean_field_name(key)
            
            if isinstance(value, (dict, list)):
                # ì¤‘ì²©ëœ ê°ì²´ë‚˜ ë°°ì—´ì¸ ê²½ìš°
                st.markdown(f"**{korean_key}:**")
                if isinstance(value, dict):
                    for sub_key, sub_value in value.items():
                        korean_sub_key = get_korean_field_name(sub_key)
                        st.markdown(f"  - {korean_sub_key}: {sub_value}")
                else:  # list
                    for item in value:
                        st.markdown(f"  - {item}")
            else:
                # ë‹¨ìˆœ ê°’ì¸ ê²½ìš°
                st.markdown(f"**{korean_key}:** {value}")
    elif isinstance(data, list):
        for i, item in enumerate(data, 1):
            st.markdown(f"{i}. {item}")
    else:
        st.markdown(str(data))

def get_korean_field_name(key):
    """ì˜ì–´ í•„ë“œëª…ì„ í•œêµ­ì–´ë¡œ ë³€í™˜"""
    field_mapping = {
        # íŒ”ë¡œì›Œ ë„¤íŠ¸ì›Œí¬ ë¶„ì„
        "followers": "íŒ”ë¡œì›Œ ìˆ˜",
        "followings": "íŒ”ë¡œì‰ ìˆ˜", 
        "inference_reason": "ì¶”ë¡  ê·¼ê±°",
        "impact_on_brand_fit": "ë¸Œëœë“œ ì í•©ì„± ì˜í–¥",
        "network_type_inference": "ë„¤íŠ¸ì›Œí¬ ìœ í˜• ì¶”ë¡ ",
        "influence_authenticity_score": "ì˜í–¥ë ¥ ì§„ì •ì„± ì ìˆ˜",
        "ratio_followers_to_followings": "íŒ”ë¡œì›Œ/íŒ”ë¡œì‰ ë¹„ìœ¨",
        
        # ëŒ“ê¸€ ì§„ì •ì„± ë¶„ì„
        "comment_authenticity": "ëŒ“ê¸€ ì§„ì •ì„±",
        "bot_detection": "ë´‡ íƒì§€",
        "engagement_quality": "ì°¸ì—¬ í’ˆì§ˆ",
        "interaction_pattern": "ìƒí˜¸ì‘ìš© íŒ¨í„´",
        
        # ì½˜í…ì¸  ë¶„ì„
        "content_theme": "ì½˜í…ì¸  í…Œë§ˆ",
        "posting_frequency": "ê²Œì‹œ ë¹ˆë„",
        "content_quality": "ì½˜í…ì¸  í’ˆì§ˆ",
        "inference_confidence": "ì¶”ë¡  ì‹ ë¢°ë„",
        "content_type": "ì½˜í…ì¸  ìœ í˜•",
        "engagement_rate": "ì°¸ì—¬ìœ¨",
        
        # ì¸ì‚¬ì´íŠ¸
        "strengths": "ê°•ì ",
        "weaknesses": "ì•½ì ",
        "opportunities": "ê¸°íšŒ",
        "threats": "ìœ„í˜‘",
        "recommendations": "ì¶”ì²œì‚¬í•­",
        
        # ê¸°íƒ€
        "notes": "ë…¸íŠ¸",
        "additional_info": "ì¶”ê°€ ì •ë³´",
        "analysis_date": "ë¶„ì„ ë‚ ì§œ",
        "confidence_level": "ì‹ ë¢°ë„ ìˆ˜ì¤€"
    }
    
    return field_mapping.get(key, key)

def show_detailed_analysis_improved(analysis):
    """ê°œì„ ëœ ìƒì„¸ ë¶„ì„ ê²°ê³¼ í‘œì‹œ - ëª¨ë“  AI ë¶„ì„ ì •ë³´ë¥¼ í‘œì‹œ"""
    
    # ê¸°ë³¸ ì •ë³´ ì„¹ì…˜
    st.markdown("### ğŸ“‹ ê¸°ë³¸ ì •ë³´")
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("ì´ë¦„", analysis.get('name', 'N/A'))
        st.metric("ë³„ëª…", analysis.get('alias', 'N/A'))
    
    with col2:
        st.metric("í”Œë«í¼", analysis.get('platform', 'N/A'))
        st.metric("ì¹´í…Œê³ ë¦¬", analysis.get('category', 'N/A'))
    
    with col3:
        # ìˆ«ì ë°ì´í„° ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
        followers = analysis.get('followers', 0)
        followings = analysis.get('followings', 0)
        try:
            followers_num = int(followers) if followers else 0
            followings_num = int(followings) if followings else 0
        except (ValueError, TypeError):
            followers_num = 0
            followings_num = 0
        
        st.metric("íŒ”ë¡œì›Œ", f"{followers_num:,}ëª…")
        st.metric("íŒ”ë¡œì‰", f"{followings_num:,}ëª…")
    
    with col4:
        # ê²Œì‹œë¬¼ ìˆ˜ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
        posts_count = analysis.get('posts_count', 0)
        try:
            posts_count_num = int(posts_count) if posts_count else 0
        except (ValueError, TypeError):
            posts_count_num = 0
            
        st.metric("ê²Œì‹œë¬¼ ìˆ˜", f"{posts_count_num:,}ê°œ")
        st.metric("ì¶”ì²œë„", analysis.get('recommendation', 'N/A'))
    
    # íƒœê·¸ í‘œì‹œ
    if analysis.get('tags'):
        st.markdown("**ğŸ·ï¸ íƒœê·¸:**")
        tags = analysis['tags'] if isinstance(analysis['tags'], list) else []
        if tags:
            tag_cols = st.columns(min(len(tags), 5))
            for i, tag in enumerate(tags[:5]):
                with tag_cols[i]:
                    st.markdown(f"`{tag}`")
        else:
            st.markdown("íƒœê·¸ ì—†ìŒ")
    
    # ìš”ì•½ ì •ë³´
    if analysis.get('summary'):
        st.markdown("### ğŸ“ AI ë¶„ì„ ìš”ì•½")
        st.info(analysis['summary'])
    
    # í‰ê°€ ì ìˆ˜ ì„¹ì…˜
    evaluation = analysis.get('evaluation', {})
    if evaluation:
        st.markdown("### â­ í‰ê°€ ì ìˆ˜")
        score_cols = st.columns(5)
        
        # ì ìˆ˜ ë°ì´í„° ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
        def safe_get_score(score_dict, key, default=0):
            try:
                value = score_dict.get(key, default)
                return float(value) if value is not None else default
            except (ValueError, TypeError):
                return default
        
        with score_cols[0]:
            engagement_score = safe_get_score(evaluation, 'engagement', 0)
            st.metric("ì°¸ì—¬ë„", f"{engagement_score}/10")
        with score_cols[1]:
            activity_score = safe_get_score(evaluation, 'activity', 0)
            st.metric("í™œë™ì„±", f"{activity_score}/10")
        with score_cols[2]:
            communication_score = safe_get_score(evaluation, 'communication', 0)
            st.metric("ì†Œí†µëŠ¥ë ¥", f"{communication_score}/10")
        with score_cols[3]:
            growth_potential_score = safe_get_score(evaluation, 'growth_potential', 0)
            st.metric("ì„±ì¥ì ì¬ë ¥", f"{growth_potential_score}/10")
        with score_cols[4]:
            overall_score = safe_get_score(evaluation, 'overall_score', 0)
            st.metric("ì¢…í•©ì ìˆ˜", f"{overall_score}/10")
    
    # ìƒì„¸ ë¶„ì„ ì„¹ì…˜ë“¤
    st.markdown("### ğŸ” ìƒì„¸ ë¶„ì„")
    
    # íŒ”ë¡œì›Œ ë„¤íŠ¸ì›Œí¬ ë¶„ì„
    if analysis.get("follow_network_analysis"):
        with st.expander("ğŸ‘¥ íŒ”ë¡œì›Œ ë„¤íŠ¸ì›Œí¬ ë¶„ì„", expanded=False):
            display_analysis_section(analysis["follow_network_analysis"])
    
    # ëŒ“ê¸€ ì§„ì •ì„± ë¶„ì„
    if analysis.get("comment_authenticity_analysis"):
        with st.expander("ğŸ’¬ ëŒ“ê¸€ ì§„ì •ì„± ë¶„ì„", expanded=False):
            display_analysis_section(analysis["comment_authenticity_analysis"])
    
    # ì½˜í…ì¸  ë¶„ì„
    if analysis.get("content_analysis"):
        with st.expander("ğŸ“ ì½˜í…ì¸  ë¶„ì„", expanded=False):
            display_analysis_section(analysis["content_analysis"])
    
    # ì¸ì‚¬ì´íŠ¸
    if analysis.get("insights"):
        with st.expander("ğŸ’¡ ì¸ì‚¬ì´íŠ¸", expanded=False):
            display_analysis_section(analysis["insights"])
    
    # ì¶”ê°€ ë…¸íŠ¸
    if analysis.get("notes"):
        with st.expander("ğŸ“‹ ì¶”ê°€ ë…¸íŠ¸", expanded=False):
            display_analysis_section(analysis["notes"])
    

def show_detailed_analysis(analysis):
    """ìƒì„¸ ë¶„ì„ ê²°ê³¼ í‘œì‹œ (ê¸°ì¡´ í•¨ìˆ˜ - í˜¸í™˜ì„± ìœ ì§€)"""
    st.markdown("### ğŸ“Š ìƒì„¸ ë¶„ì„ ê²°ê³¼")
    
    # íŒ”ë¡œì›Œ ë„¤íŠ¸ì›Œí¬ ë¶„ì„
    if analysis.get("follow_network_analysis"):
        st.markdown("**ğŸ‘¥ íŒ”ë¡œì›Œ ë„¤íŠ¸ì›Œí¬ ë¶„ì„:**")
        display_analysis_section(analysis["follow_network_analysis"])
    
    # ëŒ“ê¸€ ì§„ì •ì„± ë¶„ì„
    if analysis.get("comment_authenticity_analysis"):
        st.markdown("**ğŸ’¬ ëŒ“ê¸€ ì§„ì •ì„± ë¶„ì„:**")
        display_analysis_section(analysis["comment_authenticity_analysis"])
    
    # ì½˜í…ì¸  ë¶„ì„
    if analysis.get("content_analysis"):
        st.markdown("**ğŸ“ ì½˜í…ì¸  ë¶„ì„:**")
        display_analysis_section(analysis["content_analysis"])
    
    # ì¸ì‚¬ì´íŠ¸
    if analysis.get("insights"):
        st.markdown("**ğŸ’¡ ì¸ì‚¬ì´íŠ¸:**")
        display_analysis_section(analysis["insights"])
    
    # ì¶”ê°€ ë…¸íŠ¸
    if analysis.get("notes"):
        st.markdown("**ğŸ“‹ ì¶”ê°€ ë…¸íŠ¸:**")
        display_analysis_section(analysis["notes"])

def render_ai_analysis_statistics():
    """AI ë¶„ì„ í†µê³„ íƒ­ - ê³ ë„í™”ëœ ë²„ì „"""
    st.subheader("ğŸ“ˆ ì¸ê³µì§€ëŠ¥ ë¶„ì„ í†µê³„")
    st.markdown("AI ë¶„ì„ ê²°ê³¼ì˜ ëª¨ë“  ìˆ˜ì¹˜ ì •ë³´ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.")
    
    try:
        # ì „ì²´ í†µê³„ ê°œìš”
        render_overview_statistics()
        
        # íƒ­ìœ¼ë¡œ ë¶„ë¦¬ëœ ìƒì„¸ í†µê³„
        tab1, tab2, tab3, tab4, tab5 = st.tabs([
            "ğŸ“Š ê¸°ë³¸ ê³„ì • ì •ë³´", 
            "ğŸ” ë„¤íŠ¸ì›Œí¬ ë¶„ì„", 
            "ğŸ“ˆ í™œë™ì„±/ë°˜ì‘ì„±", 
            "ğŸ’¬ ëŒ“ê¸€ ì§„ì •ì„±", 
            "ğŸ¯ í‰ê°€ ì ìˆ˜"
        ])
        
        with tab1:
            render_basic_account_statistics()
        
        with tab2:
            render_network_analysis_statistics()
        
        with tab3:
            render_activity_metrics_statistics()
        
        with tab4:
            render_comment_authenticity_statistics()
        
        with tab5:
            render_evaluation_scores_statistics()
    
    except Exception as e:
        st.error(f"í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")

def render_overview_statistics():
    """ì „ì²´ í†µê³„ ê°œìš”"""
    st.markdown("### ğŸ“Š ì „ì²´ í†µê³„ ê°œìš”")
    
    # ê¸°ë³¸ í†µê³„
    total_analyses = get_total_analyses_count()
    recent_analyses = get_recent_analyses_count()
    avg_score = get_average_overall_score()
    recommendation_dist = get_recommendation_distribution()
    most_common = max(recommendation_dist.items(), key=lambda x: x[1])[0] if recommendation_dist else "ì—†ìŒ"
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("ì´ ë¶„ì„ ìˆ˜", f"{total_analyses:,}ê±´")
    
    with col2:
        st.metric("ìµœê·¼ 7ì¼ ë¶„ì„", f"{recent_analyses:,}ê±´")
    
    with col3:
        st.metric("í‰ê·  ì¢…í•©ì ìˆ˜", f"{avg_score:.1f}/10")
    
    with col4:
        st.metric("ê°€ì¥ ë§ì€ ì¶”ì²œë„", most_common)
    
    # ì¶”ì²œë„ ë¶„í¬ ì°¨íŠ¸
    if recommendation_dist:
        col1, col2 = st.columns([1, 1])
        with col1:
            fig = px.pie(
                values=list(recommendation_dist.values()),
                names=list(recommendation_dist.keys()),
                title="ì¶”ì²œë„ ë¶„í¬"
            )
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            category_stats = get_category_statistics()
            if category_stats:
                fig = px.bar(
                    x=list(category_stats.keys()),
                    y=list(category_stats.values()),
                    title="ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„ ìˆ˜"
                )
                st.plotly_chart(fig, use_container_width=True)

def render_basic_account_statistics():
    """ê¸°ë³¸ ê³„ì • ì •ë³´ í†µê³„"""
    st.markdown("### ğŸ“Š ê¸°ë³¸ ê³„ì • ì •ë³´ í†µê³„")
    
    try:
        # ê¸°ë³¸ ê³„ì • ì •ë³´ ì¡°íšŒ
        basic_stats = get_basic_account_statistics()
        
        if not basic_stats:
            st.warning("ê¸°ë³¸ ê³„ì • ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # íŒ”ë¡œì›Œ ìˆ˜ í†µê³„
        st.markdown("#### ğŸ‘¥ íŒ”ë¡œì›Œ ìˆ˜ ë¶„ì„")
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("í‰ê·  íŒ”ë¡œì›Œ", f"{basic_stats['avg_followers']:,.0f}ëª…")
        with col2:
            st.metric("ì¤‘ì•™ê°’ íŒ”ë¡œì›Œ", f"{basic_stats['median_followers']:,.0f}ëª…")
        with col3:
            st.metric("ìµœëŒ€ íŒ”ë¡œì›Œ", f"{basic_stats['max_followers']:,.0f}ëª…")
        with col4:
            st.metric("ìµœì†Œ íŒ”ë¡œì›Œ", f"{basic_stats['min_followers']:,.0f}ëª…")
        
        # íŒ”ë¡œì›Œ ë¶„í¬ ì°¨íŠ¸
        if basic_stats['followers_distribution']:
            fig = px.histogram(
                x=basic_stats['followers_distribution'],
                nbins=20,
                title="íŒ”ë¡œì›Œ ìˆ˜ ë¶„í¬",
                labels={"x": "íŒ”ë¡œì›Œ ìˆ˜", "y": "ì¸í”Œë£¨ì–¸ì„œ ìˆ˜"}
            )
            st.plotly_chart(fig, use_container_width=True)
        
        # íŒ”ë¡œì‰ ìˆ˜ í†µê³„
        st.markdown("#### ğŸ‘¤ íŒ”ë¡œì‰ ìˆ˜ ë¶„ì„")
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("í‰ê·  íŒ”ë¡œì‰", f"{basic_stats['avg_followings']:,.0f}ëª…")
        with col2:
            st.metric("ì¤‘ì•™ê°’ íŒ”ë¡œì‰", f"{basic_stats['median_followings']:,.0f}ëª…")
        with col3:
            st.metric("ìµœëŒ€ íŒ”ë¡œì‰", f"{basic_stats['max_followings']:,.0f}ëª…")
        with col4:
            st.metric("ìµœì†Œ íŒ”ë¡œì‰", f"{basic_stats['min_followings']:,.0f}ëª…")
        
        # ê²Œì‹œë¬¼ ìˆ˜ í†µê³„
        st.markdown("#### ğŸ“ ê²Œì‹œë¬¼ ìˆ˜ ë¶„ì„")
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("í‰ê·  ê²Œì‹œë¬¼", f"{basic_stats['avg_posts']:,.0f}ê°œ")
        with col2:
            st.metric("ì¤‘ì•™ê°’ ê²Œì‹œë¬¼", f"{basic_stats['median_posts']:,.0f}ê°œ")
        with col3:
            st.metric("ìµœëŒ€ ê²Œì‹œë¬¼", f"{basic_stats['max_posts']:,.0f}ê°œ")
        with col4:
            st.metric("ìµœì†Œ ê²Œì‹œë¬¼", f"{basic_stats['min_posts']:,.0f}ê°œ")
        
        # íŒ”ë¡œì›Œ/íŒ”ë¡œì‰ ë¹„ìœ¨ ë¶„ì„
        st.markdown("#### âš–ï¸ íŒ”ë¡œì›Œ/íŒ”ë¡œì‰ ë¹„ìœ¨ ë¶„ì„")
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("í‰ê·  ë¹„ìœ¨", f"{basic_stats['avg_ratio']:.2f}")
        with col2:
            st.metric("ì¤‘ì•™ê°’ ë¹„ìœ¨", f"{basic_stats['median_ratio']:.2f}")
        with col3:
            st.metric("ìµœëŒ€ ë¹„ìœ¨", f"{basic_stats['max_ratio']:.2f}")
        with col4:
            st.metric("ìµœì†Œ ë¹„ìœ¨", f"{basic_stats['min_ratio']:.2f}")
        
        # ë¹„ìœ¨ ë¶„í¬ ì°¨íŠ¸
        if basic_stats['ratio_distribution']:
            fig = px.histogram(
                x=basic_stats['ratio_distribution'],
                nbins=20,
                title="íŒ”ë¡œì›Œ/íŒ”ë¡œì‰ ë¹„ìœ¨ ë¶„í¬",
                labels={"x": "ë¹„ìœ¨", "y": "ì¸í”Œë£¨ì–¸ì„œ ìˆ˜"}
            )
            st.plotly_chart(fig, use_container_width=True)
        
        # ìˆ˜ì¹˜ í•´ì„ ê°€ì´ë“œ
        with st.expander("ğŸ“– ìˆ˜ì¹˜ í•´ì„ ê°€ì´ë“œ", expanded=False):
            st.markdown("""
            **íŒ”ë¡œì›Œ ìˆ˜ í•´ì„:**
            - 1ë§Œëª… ë¯¸ë§Œ: ë§ˆì´í¬ë¡œ ì¸í”Œë£¨ì–¸ì„œ
            - 1ë§Œ~10ë§Œëª…: ì†Œê·œëª¨ ì¸í”Œë£¨ì–¸ì„œ  
            - 10ë§Œ~100ë§Œëª…: ì¤‘ê°„ ê·œëª¨ ì¸í”Œë£¨ì–¸ì„œ
            - 100ë§Œëª… ì´ìƒ: ëŒ€í˜• ì¸í”Œë£¨ì–¸ì„œ
            
            **íŒ”ë¡œì›Œ/íŒ”ë¡œì‰ ë¹„ìœ¨ í•´ì„:**
            - 1.0~1.3: ìƒí˜¸ íŒ”ë¡œìš°í˜• (í’ˆì•—ì´í˜•)
            - 0.5~1.0: ê· í˜•í˜•
            - 0.5 ë¯¸ë§Œ: ì˜í–¥ë ¥ ì§‘ì¤‘í˜• (ì§„ì •ì„± ë†’ìŒ)
            """)
    
    except Exception as e:
        st.error(f"ê¸°ë³¸ ê³„ì • ì •ë³´ í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")

def render_network_analysis_statistics():
    """ë„¤íŠ¸ì›Œí¬ ë¶„ì„ í†µê³„"""
    st.markdown("### ğŸ” ë„¤íŠ¸ì›Œí¬ ë¶„ì„ í†µê³„")
    
    try:
        # ë„¤íŠ¸ì›Œí¬ ë¶„ì„ í†µê³„ ì¡°íšŒ
        network_stats = get_network_analysis_statistics()
        
        if not network_stats:
            st.warning("ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ì˜í–¥ë ¥ ì§„ì •ì„± ì ìˆ˜ ë¶„ì„
        st.markdown("#### ğŸ¯ ì˜í–¥ë ¥ ì§„ì •ì„± ì ìˆ˜ ë¶„ì„")
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("í‰ê·  ì§„ì •ì„± ì ìˆ˜", f"{network_stats['avg_authenticity_score']:.1f}/100")
        with col2:
            st.metric("ì¤‘ì•™ê°’ ì§„ì •ì„± ì ìˆ˜", f"{network_stats['median_authenticity_score']:.1f}/100")
        with col3:
            st.metric("ìµœê³  ì§„ì •ì„± ì ìˆ˜", f"{network_stats['max_authenticity_score']:.1f}/100")
        with col4:
            st.metric("ìµœì € ì§„ì •ì„± ì ìˆ˜", f"{network_stats['min_authenticity_score']:.1f}/100")
        
        # ì§„ì •ì„± ì ìˆ˜ ë¶„í¬ ì°¨íŠ¸
        if network_stats['authenticity_distribution']:
            fig = px.histogram(
                x=network_stats['authenticity_distribution'],
                nbins=20,
                title="ì˜í–¥ë ¥ ì§„ì •ì„± ì ìˆ˜ ë¶„í¬",
                labels={"x": "ì§„ì •ì„± ì ìˆ˜", "y": "ì¸í”Œë£¨ì–¸ì„œ ìˆ˜"}
            )
            st.plotly_chart(fig, use_container_width=True)
        
        # ë„¤íŠ¸ì›Œí¬ ìœ í˜• ë¶„í¬
        if network_stats['network_type_distribution']:
            st.markdown("#### ğŸŒ ë„¤íŠ¸ì›Œí¬ ìœ í˜• ë¶„í¬")
            fig = px.pie(
                values=list(network_stats['network_type_distribution'].values()),
                names=list(network_stats['network_type_distribution'].keys()),
                title="ë„¤íŠ¸ì›Œí¬ ìœ í˜• ë¶„í¬"
            )
            st.plotly_chart(fig, use_container_width=True)
        
        # ìˆ˜ì¹˜ í•´ì„ ê°€ì´ë“œ
        with st.expander("ğŸ“– ë„¤íŠ¸ì›Œí¬ ë¶„ì„ í•´ì„ ê°€ì´ë“œ", expanded=False):
            st.markdown("""
            **ì˜í–¥ë ¥ ì§„ì •ì„± ì ìˆ˜ í•´ì„:**
            - 80~100ì : ì •ìƒ ì¸í”Œë£¨ì–¸ì„œí˜• (ì‹¤ì œ íŒ¬ì¸µ ì¤‘ì‹¬)
            - 60~79ì : ê· í˜•í˜• (ì¼ë¶€ ìƒí˜¸ íŒ”ë¡œìš° í¬í•¨)
            - 40~59ì : í’ˆì•—ì´í˜• (ìƒí˜¸ íŒ”ë¡œìš° ìœ„ì£¼)
            - 40ì  ë¯¸ë§Œ: ì˜ì‹¬ìŠ¤ëŸ¬ìš´ íŒ¨í„´
            
            **ë„¤íŠ¸ì›Œí¬ ìœ í˜•:**
            - ì •ìƒ ì¸í”Œë£¨ì–¸ì„œí˜•: íŒ”ë¡œì›Œ ëŒ€ë¹„ íŒ”ë¡œì‰ ë¹„ìœ¨ì´ ë‚®ìŒ
            - í’ˆì•—ì´í˜•: íŒ”ë¡œì›Œì™€ íŒ”ë¡œì‰ ìˆ˜ê°€ ë¹„ìŠ·í•¨
            - ê· í˜•í˜•: ì¤‘ê°„ ì •ë„ì˜ íŒ”ë¡œìš° íŒ¨í„´
            """)
    
    except Exception as e:
        st.error(f"ë„¤íŠ¸ì›Œí¬ ë¶„ì„ í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")

def render_activity_metrics_statistics():
    """í™œë™ì„±/ë°˜ì‘ì„± ë©”íŠ¸ë¦­ í†µê³„"""
    st.markdown("### ğŸ“ˆ í™œë™ì„±/ë°˜ì‘ì„± ë©”íŠ¸ë¦­ í†µê³„")
    
    try:
        # í™œë™ì„± ë©”íŠ¸ë¦­ í†µê³„ ì¡°íšŒ
        activity_stats = get_activity_metrics_statistics()
        
        if not activity_stats:
            st.warning("í™œë™ì„± ë©”íŠ¸ë¦­ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ìµœê·¼ 5ê°œ í¬ìŠ¤íŠ¸ í†µê³„
        st.markdown("#### ğŸ“Š ìµœê·¼ 5ê°œ í¬ìŠ¤íŠ¸ ë¶„ì„")
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("í‰ê·  ì¢‹ì•„ìš”", f"{activity_stats['avg_likes']:,.0f}")
        with col2:
            st.metric("ì¤‘ì•™ê°’ ì¢‹ì•„ìš”", f"{activity_stats['median_likes']:,.0f}")
        with col3:
            st.metric("í‰ê·  ëŒ“ê¸€", f"{activity_stats['avg_comments']:,.0f}")
        with col4:
            st.metric("í‰ê·  ì°¸ì—¬ìœ¨", f"{activity_stats['avg_engagement_rate']:.2f}%")
        
        # í™œë™ ì£¼ê¸° ë¶„ì„
        st.markdown("#### â° í™œë™ ì£¼ê¸° ë¶„ì„")
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("í‰ê·  í™œë™ ì£¼ê¸°", f"{activity_stats['avg_recency_span']:.1f}ì¼")
        with col2:
            st.metric("ì¤‘ì•™ê°’ í™œë™ ì£¼ê¸°", f"{activity_stats['median_recency_span']:.1f}ì¼")
        with col3:
            st.metric("ìµœë‹¨ í™œë™ ì£¼ê¸°", f"{activity_stats['min_recency_span']:.1f}ì¼")
        with col4:
            st.metric("ìµœì¥ í™œë™ ì£¼ê¸°", f"{activity_stats['max_recency_span']:.1f}ì¼")
        
        # ê²Œì‹œ ë¹ˆë„ ë¶„ì„
        if activity_stats['posting_pace_distribution']:
            st.markdown("#### ğŸ“… ê²Œì‹œ ë¹ˆë„ ë¶„í¬")
            fig = px.pie(
                values=list(activity_stats['posting_pace_distribution'].values()),
                names=list(activity_stats['posting_pace_distribution'].keys()),
                title="ê²Œì‹œ ë¹ˆë„ ë¶„í¬"
            )
            st.plotly_chart(fig, use_container_width=True)
        
        # ì°¸ì—¬ìœ¨ ë¶„í¬ ì°¨íŠ¸
        if activity_stats['engagement_rate_distribution']:
            fig = px.histogram(
                x=activity_stats['engagement_rate_distribution'],
                nbins=20,
                title="ì°¸ì—¬ìœ¨ ë¶„í¬",
                labels={"x": "ì°¸ì—¬ìœ¨ (%)", "y": "ì¸í”Œë£¨ì–¸ì„œ ìˆ˜"}
            )
            st.plotly_chart(fig, use_container_width=True)
        
        # ìˆ˜ì¹˜ í•´ì„ ê°€ì´ë“œ
        with st.expander("ğŸ“– í™œë™ì„± ë©”íŠ¸ë¦­ í•´ì„ ê°€ì´ë“œ", expanded=False):
            st.markdown("""
            **ì°¸ì—¬ìœ¨ í•´ì„:**
            - 6% ì´ìƒ: ë§¤ìš° í™œë°œí•œ ì°¸ì—¬
            - 3~6%: ìš°ìˆ˜í•œ ì°¸ì—¬
            - 1~3%: ì¼ë°˜ì ì¸ ì°¸ì—¬
            - 1% ë¯¸ë§Œ: ë‚®ì€ ì°¸ì—¬
            
            **ê²Œì‹œ ë¹ˆë„ í•´ì„:**
            - ë§¤ìš° ë†’ìŒ: 4ê°œ ì´ìƒ & 7ì¼ ì´ë‚´
            - ë†’ìŒ: 3ê°œ ì´ìƒ & 14ì¼ ì´ë‚´
            - ë³´í†µ: 2ê°œ ì´ìƒ & 30ì¼ ì´ë‚´
            - ë‚®ìŒ: 1ê°œ & 30ì¼ ì´ë‚´
            - ë¶ˆëª…: ë°ì´í„° ë¶€ì¡±
            
            **í™œë™ ì£¼ê¸° í•´ì„:**
            - 7ì¼ ì´ë‚´: ë§¤ìš° í™œë°œí•œ í™œë™
            - 7~14ì¼: í™œë°œí•œ í™œë™
            - 14~30ì¼: ë³´í†µ í™œë™
            - 30ì¼ ì´ìƒ: ë‚®ì€ í™œë™
            """)
    
    except Exception as e:
        st.error(f"í™œë™ì„± ë©”íŠ¸ë¦­ í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")

def render_comment_authenticity_statistics():
    """ëŒ“ê¸€ ì§„ì •ì„± ë¶„ì„ í†µê³„"""
    st.markdown("### ğŸ’¬ ëŒ“ê¸€ ì§„ì •ì„± ë¶„ì„ í†µê³„")
    
    try:
        # ëŒ“ê¸€ ì§„ì •ì„± í†µê³„ ì¡°íšŒ
        comment_stats = get_comment_authenticity_statistics()
        
        if not comment_stats:
            st.warning("ëŒ“ê¸€ ì§„ì •ì„± ë¶„ì„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ëŒ“ê¸€ ì§„ì •ì„± ë¹„ìœ¨ ë¶„ì„
        st.markdown("#### ğŸ¯ ëŒ“ê¸€ ì§„ì •ì„± ë¹„ìœ¨ ë¶„ì„")
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("í‰ê·  ì§„ì •ì„± ë¹„ìœ¨", f"{comment_stats['avg_authentic_ratio']:.1f}%")
        with col2:
            st.metric("ì¤‘ì•™ê°’ ì§„ì •ì„± ë¹„ìœ¨", f"{comment_stats['median_authentic_ratio']:.1f}%")
        with col3:
            st.metric("í‰ê·  í˜•ì‹ì  ëŒ“ê¸€ ë¹„ìœ¨", f"{comment_stats['avg_low_authentic_ratio']:.1f}%")
        with col4:
            st.metric("ì¤‘ì•™ê°’ í˜•ì‹ì  ëŒ“ê¸€ ë¹„ìœ¨", f"{comment_stats['median_low_authentic_ratio']:.1f}%")
        
        # ì§„ì •ì„± ê¸°ì¤€ë³„ ë¶„í¬ ë¶„ì„
        if comment_stats['authentic_ratio_distribution']:
            st.markdown("#### ğŸ“Š ì§„ì •ì„± ê¸°ì¤€ë³„ ë¶„í¬")
            
            # ì§„ì •ì„± ê¸°ì¤€ë³„ ë¶„ë¥˜
            high_authenticity = [x for x in comment_stats['authentic_ratio_distribution'] if x >= 70]
            normal_authenticity = [x for x in comment_stats['authentic_ratio_distribution'] if 50 <= x < 70]
            low_authenticity = [x for x in comment_stats['authentic_ratio_distribution'] if 30 <= x < 50]
            very_low_authenticity = [x for x in comment_stats['authentic_ratio_distribution'] if x < 30]
            
            authenticity_categories = {
                "ë†’ì€ ì§„ì •ì„± (70% ì´ìƒ)": len(high_authenticity),
                "ë³´í†µ ì§„ì •ì„± (50~70%)": len(normal_authenticity),
                "ë‚®ì€ ì§„ì •ì„± (30~50%)": len(low_authenticity),
                "ë§¤ìš° ë‚®ì€ ì§„ì •ì„± (30% ë¯¸ë§Œ)": len(very_low_authenticity)
            }
            
            col1, col2 = st.columns([1, 1])
            
            with col1:
                # ì§„ì •ì„± ê¸°ì¤€ë³„ ë¶„í¬ íŒŒì´ ì°¨íŠ¸
                fig = px.pie(
                    values=list(authenticity_categories.values()),
                    names=list(authenticity_categories.keys()),
                    title="ì§„ì •ì„± ê¸°ì¤€ë³„ ë¶„í¬",
                    color_discrete_sequence=['#2E8B57', '#FFD700', '#FF8C00', '#DC143C']
                )
                st.plotly_chart(fig, use_container_width=True)
            
            with col2:
                # í˜•ì‹ì  ëŒ“ê¸€ ë¹„ìœ¨ ë¶„í¬
                if comment_stats['low_authentic_ratio_distribution']:
                    # í˜•ì‹ì  ëŒ“ê¸€ ë¹„ìœ¨ ê¸°ì¤€ë³„ ë¶„ë¥˜
                    good_quality = [x for x in comment_stats['low_authentic_ratio_distribution'] if x < 30]
                    normal_quality = [x for x in comment_stats['low_authentic_ratio_distribution'] if 30 <= x < 50]
                    poor_quality = [x for x in comment_stats['low_authentic_ratio_distribution'] if x >= 50]
                    
                    quality_categories = {
                        "ì–‘í˜¸í•œ í’ˆì§ˆ (30% ë¯¸ë§Œ)": len(good_quality),
                        "ë³´í†µ í’ˆì§ˆ (30~50%)": len(normal_quality),
                        "ë‚®ì€ í’ˆì§ˆ (50% ì´ìƒ)": len(poor_quality)
                    }
                    
                    fig = px.pie(
                        values=list(quality_categories.values()),
                        names=list(quality_categories.keys()),
                        title="ëŒ“ê¸€ í’ˆì§ˆ ê¸°ì¤€ë³„ ë¶„í¬",
                        color_discrete_sequence=['#2E8B57', '#FFD700', '#DC143C']
                    )
                    st.plotly_chart(fig, use_container_width=True)
        
        # ê²½ê³  ë° ì£¼ì˜ì‚¬í•­
        st.markdown("#### âš ï¸ ì£¼ì˜ì‚¬í•­ ë° ê²½ê³ ")
        
        # ì˜ì‹¬ìŠ¤ëŸ¬ìš´ íŒ¨í„´ ê°ì§€
        suspicious_count = len([x for x in comment_stats['authentic_ratio_distribution'] if x < 30])
        poor_quality_count = len([x for x in comment_stats['low_authentic_ratio_distribution'] if x >= 50]) if comment_stats['low_authentic_ratio_distribution'] else 0
        
        col1, col2 = st.columns(2)
        
        with col1:
            if suspicious_count > 0:
                st.error(f"ğŸš¨ **ì˜ì‹¬ìŠ¤ëŸ¬ìš´ íŒ¨í„´ ê°ì§€**: {suspicious_count}ëª…ì˜ ì¸í”Œë£¨ì–¸ì„œê°€ ë§¤ìš° ë‚®ì€ ì§„ì •ì„±(30% ë¯¸ë§Œ)ì„ ë³´ì…ë‹ˆë‹¤.")
            else:
                st.success("âœ… ì˜ì‹¬ìŠ¤ëŸ¬ìš´ íŒ¨í„´ì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        with col2:
            if poor_quality_count > 0:
                st.warning(f"âš ï¸ **ëŒ“ê¸€ í’ˆì§ˆ ì£¼ì˜**: {poor_quality_count}ëª…ì˜ ì¸í”Œë£¨ì–¸ì„œê°€ ë‚®ì€ ëŒ“ê¸€ í’ˆì§ˆ(50% ì´ìƒ í˜•ì‹ì  ëŒ“ê¸€)ì„ ë³´ì…ë‹ˆë‹¤.")
            else:
                st.success("âœ… ëŒ“ê¸€ í’ˆì§ˆì´ ì–‘í˜¸í•©ë‹ˆë‹¤.")
        
        # ì§„ì •ì„± ë“±ê¸‰ ë¶„í¬ì™€ ìƒìœ„/í•˜ìœ„ ì¸í”Œë£¨ì–¸ì„œ ë¶„í¬ë„
        st.markdown("#### ğŸ“ˆ AI ë¶„ì„ ë“±ê¸‰ ë¶„í¬ & ìƒìœ„/í•˜ìœ„ ì¸í”Œë£¨ì–¸ì„œ ë¶„í¬")
        
        col1, col2 = st.columns(2)
        
        with col1:
            if comment_stats['authenticity_level_distribution']:
                fig = px.pie(
                    values=list(comment_stats['authenticity_level_distribution'].values()),
                    names=list(comment_stats['authenticity_level_distribution'].keys()),
                    title="AI ë¶„ì„ ì§„ì •ì„± ë“±ê¸‰ ë¶„í¬"
                )
                st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            # ìƒìœ„/í•˜ìœ„ ì¸í”Œë£¨ì–¸ì„œ ë¶„í¬ë„
            render_top_bottom_distribution_plot()
        
        # ìƒìœ„/í•˜ìœ„ ì¸í”Œë£¨ì–¸ì„œ ìš”ì•½ í†µê³„
        st.markdown("#### ğŸ† ìƒìœ„/í•˜ìœ„ ì¸í”Œë£¨ì–¸ì„œ ìš”ì•½")
        
        # ìƒìœ„/í•˜ìœ„ ì¸í”Œë£¨ì–¸ì„œ ë°ì´í„° ì¡°íšŒ
        top_bottom_analysis = get_top_bottom_authenticity_analysis()
        
        if top_bottom_analysis:
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric("ìƒìœ„ ì¸í”Œë£¨ì–¸ì„œ ìˆ˜", f"{len(top_bottom_analysis['top_influencers'])}ëª…")
            with col2:
                st.metric("í•˜ìœ„ ì¸í”Œë£¨ì–¸ì„œ ìˆ˜", f"{len(top_bottom_analysis['bottom_influencers'])}ëª…")
            with col3:
                if top_bottom_analysis['top_influencers']:
                    avg_top = sum([x['authentic_ratio'] for x in top_bottom_analysis['top_influencers']]) / len(top_bottom_analysis['top_influencers'])
                    st.metric("ìƒìœ„ í‰ê·  ì§„ì •ì„±", f"{avg_top:.1f}%")
                else:
                    st.metric("ìƒìœ„ í‰ê·  ì§„ì •ì„±", "N/A")
            with col4:
                if top_bottom_analysis['bottom_influencers']:
                    avg_bottom = sum([x['authentic_ratio'] for x in top_bottom_analysis['bottom_influencers']]) / len(top_bottom_analysis['bottom_influencers'])
                    st.metric("í•˜ìœ„ í‰ê·  ì§„ì •ì„±", f"{avg_bottom:.1f}%")
                else:
                    st.metric("í•˜ìœ„ í‰ê·  ì§„ì •ì„±", "N/A")
        
        # ìƒì„¸ í†µê³„ í…Œì´ë¸”
        st.markdown("#### ğŸ“‹ ìƒì„¸ í†µê³„")
        
        # ì§„ì •ì„± ê¸°ì¤€ë³„ ìƒì„¸ í†µê³„
        stats_data = []
        if comment_stats['authentic_ratio_distribution']:
            stats_data.append({
                "êµ¬ë¶„": "ì§„ì •ì„± ë¹„ìœ¨",
                "í‰ê· ": f"{comment_stats['avg_authentic_ratio']:.1f}%",
                "ì¤‘ì•™ê°’": f"{comment_stats['median_authentic_ratio']:.1f}%",
                "ìµœê³ ": f"{max(comment_stats['authentic_ratio_distribution']):.1f}%",
                "ìµœì €": f"{min(comment_stats['authentic_ratio_distribution']):.1f}%"
            })
        
        if comment_stats['low_authentic_ratio_distribution']:
            stats_data.append({
                "êµ¬ë¶„": "í˜•ì‹ì  ëŒ“ê¸€ ë¹„ìœ¨",
                "í‰ê· ": f"{comment_stats['avg_low_authentic_ratio']:.1f}%",
                "ì¤‘ì•™ê°’": f"{comment_stats['median_low_authentic_ratio']:.1f}%",
                "ìµœê³ ": f"{max(comment_stats['low_authentic_ratio_distribution']):.1f}%",
                "ìµœì €": f"{min(comment_stats['low_authentic_ratio_distribution']):.1f}%"
            })
        
        if stats_data:
            st.dataframe(pd.DataFrame(stats_data), use_container_width=True)
        
        # ìˆ˜ì¹˜ í•´ì„ ê°€ì´ë“œ
        with st.expander("ğŸ“– ëŒ“ê¸€ ì§„ì •ì„± í•´ì„ ê°€ì´ë“œ", expanded=False):
            st.markdown("""
            **ëŒ“ê¸€ ì§„ì •ì„± ë¹„ìœ¨ í•´ì„:**
            - 70% ì´ìƒ: ë†’ì€ ì§„ì •ì„± (ì‹¤ì œ íŒ¬ì¸µ ì¤‘ì‹¬)
            - 50~70%: ë³´í†µ ì§„ì •ì„± (ì¼ë¶€ í˜•ì‹ì  ëŒ“ê¸€ í¬í•¨)
            - 30~50%: ë‚®ì€ ì§„ì •ì„± (ë§ì€ í˜•ì‹ì  ëŒ“ê¸€)
            - 30% ë¯¸ë§Œ: ë§¤ìš° ë‚®ì€ ì§„ì •ì„± (ì˜ì‹¬ìŠ¤ëŸ¬ìš´ íŒ¨í„´)
            
            **í˜•ì‹ì  ëŒ“ê¸€ ë¹„ìœ¨ í•´ì„:**
            - 30% ë¯¸ë§Œ: ì–‘í˜¸í•œ ëŒ“ê¸€ í’ˆì§ˆ
            - 30~50%: ë³´í†µ ëŒ“ê¸€ í’ˆì§ˆ
            - 50% ì´ìƒ: ë‚®ì€ ëŒ“ê¸€ í’ˆì§ˆ (í’ˆì•—ì´, ë´‡ ì˜ì‹¬)
            
            **ì§„ì •ì„± ë“±ê¸‰:**
            - ë†’ìŒ: ì‹¤ì œ íŒ¬ë“¤ì˜ ì§„ì •í•œ ë°˜ì‘ì´ ë§ìŒ
            - ì¤‘ê°„: ì¼ë¶€ ì§„ì •í•œ ë°˜ì‘ê³¼ í˜•ì‹ì  ëŒ“ê¸€ í˜¼ì¬
            - ë‚®ìŒ: ëŒ€ë¶€ë¶„ í˜•ì‹ì ì´ê±°ë‚˜ ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ëŒ“ê¸€
            """)
    
    except Exception as e:
        st.error(f"ëŒ“ê¸€ ì§„ì •ì„± í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")

def render_evaluation_scores_statistics():
    """í‰ê°€ ì ìˆ˜ í†µê³„"""
    st.markdown("### ğŸ¯ í‰ê°€ ì ìˆ˜ í†µê³„")
    
    try:
        # í‰ê°€ ì ìˆ˜ í†µê³„ ì¡°íšŒ
        score_stats = get_evaluation_scores_statistics()
        
        if not score_stats:
            st.warning("í‰ê°€ ì ìˆ˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ê° ì ìˆ˜ë³„ í†µê³„
        st.markdown("#### ğŸ“Š ì ìˆ˜ë³„ í†µê³„")
        
        # ì°¸ì—¬ë„ ì ìˆ˜
        st.markdown("##### ğŸ’¬ ì°¸ì—¬ë„ ì ìˆ˜")
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("í‰ê·  ì°¸ì—¬ë„", f"{score_stats['avg_engagement']:.1f}/10")
        with col2:
            st.metric("ì¤‘ì•™ê°’ ì°¸ì—¬ë„", f"{score_stats['median_engagement']:.1f}/10")
        with col3:
            st.metric("ìµœê³  ì°¸ì—¬ë„", f"{score_stats['max_engagement']:.1f}/10")
        with col4:
            st.metric("ìµœì € ì°¸ì—¬ë„", f"{score_stats['min_engagement']:.1f}/10")
        
        # í™œë™ì„± ì ìˆ˜
        st.markdown("##### ğŸƒ í™œë™ì„± ì ìˆ˜")
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("í‰ê·  í™œë™ì„±", f"{score_stats['avg_activity']:.1f}/10")
        with col2:
            st.metric("ì¤‘ì•™ê°’ í™œë™ì„±", f"{score_stats['median_activity']:.1f}/10")
        with col3:
            st.metric("ìµœê³  í™œë™ì„±", f"{score_stats['max_activity']:.1f}/10")
        with col4:
            st.metric("ìµœì € í™œë™ì„±", f"{score_stats['min_activity']:.1f}/10")
        
        # ì†Œí†µëŠ¥ë ¥ ì ìˆ˜
        st.markdown("##### ğŸ’­ ì†Œí†µëŠ¥ë ¥ ì ìˆ˜")
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("í‰ê·  ì†Œí†µëŠ¥ë ¥", f"{score_stats['avg_communication']:.1f}/10")
        with col2:
            st.metric("ì¤‘ì•™ê°’ ì†Œí†µëŠ¥ë ¥", f"{score_stats['median_communication']:.1f}/10")
        with col3:
            st.metric("ìµœê³  ì†Œí†µëŠ¥ë ¥", f"{score_stats['max_communication']:.1f}/10")
        with col4:
            st.metric("ìµœì € ì†Œí†µëŠ¥ë ¥", f"{score_stats['min_communication']:.1f}/10")
        
        # ì„±ì¥ì ì¬ë ¥ ì ìˆ˜
        st.markdown("##### ğŸŒ± ì„±ì¥ì ì¬ë ¥ ì ìˆ˜")
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("í‰ê·  ì„±ì¥ì ì¬ë ¥", f"{score_stats['avg_growth_potential']:.1f}/10")
        with col2:
            st.metric("ì¤‘ì•™ê°’ ì„±ì¥ì ì¬ë ¥", f"{score_stats['median_growth_potential']:.1f}/10")
        with col3:
            st.metric("ìµœê³  ì„±ì¥ì ì¬ë ¥", f"{score_stats['max_growth_potential']:.1f}/10")
        with col4:
            st.metric("ìµœì € ì„±ì¥ì ì¬ë ¥", f"{score_stats['min_growth_potential']:.1f}/10")
        
        # ì¢…í•©ì ìˆ˜
        st.markdown("##### ğŸ† ì¢…í•©ì ìˆ˜")
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("í‰ê·  ì¢…í•©ì ìˆ˜", f"{score_stats['avg_overall']:.1f}/10")
        with col2:
            st.metric("ì¤‘ì•™ê°’ ì¢…í•©ì ìˆ˜", f"{score_stats['median_overall']:.1f}/10")
        with col3:
            st.metric("ìµœê³  ì¢…í•©ì ìˆ˜", f"{score_stats['max_overall']:.1f}/10")
        with col4:
            st.metric("ìµœì € ì¢…í•©ì ìˆ˜", f"{score_stats['min_overall']:.1f}/10")
        
        # ì ìˆ˜ ë¶„í¬ ì°¨íŠ¸
        if score_stats['score_distributions']:
            col1, col2 = st.columns(2)
            
            with col1:
                # ì¢…í•©ì ìˆ˜ ë¶„í¬
                fig = px.histogram(
                    x=score_stats['score_distributions']['overall'],
                    nbins=10,
                    title="ì¢…í•©ì ìˆ˜ ë¶„í¬",
                    labels={"x": "ì¢…í•©ì ìˆ˜", "y": "ì¸í”Œë£¨ì–¸ì„œ ìˆ˜"}
                )
                st.plotly_chart(fig, use_container_width=True)
            
            with col2:
                # ì¶”ë¡  ì‹ ë¢°ë„ ë¶„í¬
                if score_stats['inference_confidence_distribution']:
                    fig = px.histogram(
                        x=score_stats['inference_confidence_distribution'],
                        nbins=10,
                        title="ì¶”ë¡  ì‹ ë¢°ë„ ë¶„í¬",
                        labels={"x": "ì¶”ë¡  ì‹ ë¢°ë„", "y": "ì¸í”Œë£¨ì–¸ì„œ ìˆ˜"}
                    )
                    st.plotly_chart(fig, use_container_width=True)
        
        # ì ìˆ˜ ìƒê´€ê´€ê³„ ë¶„ì„
        if score_stats['correlation_data']:
            st.markdown("#### ğŸ”— ì ìˆ˜ ìƒê´€ê´€ê³„ ë¶„ì„")
            fig = px.imshow(
                score_stats['correlation_data'],
                title="ì ìˆ˜ ê°„ ìƒê´€ê´€ê³„",
                color_continuous_scale='RdBu_r'
            )
            st.plotly_chart(fig, use_container_width=True)
        
        # ìˆ˜ì¹˜ í•´ì„ ê°€ì´ë“œ
        with st.expander("ğŸ“– í‰ê°€ ì ìˆ˜ í•´ì„ ê°€ì´ë“œ", expanded=False):
            st.markdown("""
            **ì ìˆ˜ í•´ì„ ê¸°ì¤€:**
            - 9~10ì : ë§¤ìš° ìš°ìˆ˜
            - 7~8ì : ìš°ìˆ˜
            - 5~6ì : ë³´í†µ
            - 3~4ì : ë¯¸í¡
            - 0~2ì : ë§¤ìš° ë¯¸í¡
            
            **ê° ì ìˆ˜ ì˜ë¯¸:**
            - ì°¸ì—¬ë„: íŒ”ë¡œì›Œë“¤ì˜ ë°˜ì‘ ê°•ë„ì™€ ì¼ê´€ì„±
            - í™œë™ì„±: ê²Œì‹œ ë¹ˆë„ì™€ ê¾¸ì¤€í•¨
            - ì†Œí†µëŠ¥ë ¥: íŒ¬ê³¼ì˜ ì‹¤ì§ˆì ì¸ ìƒí˜¸ì‘ìš© ì •ë„
            - ì„±ì¥ì ì¬ë ¥: ë¸Œëœë“œ í˜‘ì—… ë° í™•ì¥ ê°€ëŠ¥ì„±
            - ì¢…í•©ì ìˆ˜: ìœ„ 4ê°œ ì ìˆ˜ì˜ í‰ê· 
            
            **ì¶”ë¡  ì‹ ë¢°ë„:**
            - 0.8~1.0: ë§¤ìš° ë†’ì€ ì‹ ë¢°ë„
            - 0.6~0.8: ë†’ì€ ì‹ ë¢°ë„
            - 0.4~0.6: ë³´í†µ ì‹ ë¢°ë„
            - 0.2~0.4: ë‚®ì€ ì‹ ë¢°ë„
            - 0.0~0.2: ë§¤ìš° ë‚®ì€ ì‹ ë¢°ë„
            """)
    
    except Exception as e:
        st.error(f"í‰ê°€ ì ìˆ˜ í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")

def get_total_analyses_count():
    """ì´ ë¶„ì„ ìˆ˜ ì¡°íšŒ - ì¬ì‹œë„ ë¡œì§ í¬í•¨"""
    max_retries = 3
    retry_delay = 1
    
    for attempt in range(max_retries):
        try:
            client = simple_client.get_client()
            if not client:
                return 0
            
            response = client.table("ai_influencer_analyses").select("id", count="exact").execute()
            return response.count if response.count else 0
            
        except Exception as e:
            error_msg = str(e)
            if "Server disconnected" in error_msg or "connection" in error_msg.lower():
                if attempt < max_retries - 1:
                    time.sleep(retry_delay)
                    retry_delay *= 2
                    continue
                else:
                    return 0
            else:
                return 0
    
    return 0

def get_recent_analyses_count():
    """ìµœê·¼ 7ì¼ ë¶„ì„ ìˆ˜ ì¡°íšŒ"""
    try:
        seven_days_ago = datetime.now() - timedelta(days=7)
        client = simple_client.get_client()
        if not client:
            return 0
        
        response = client.table("ai_influencer_analyses").select("id", count="exact").gte("analyzed_at", seven_days_ago.isoformat()).execute()
        return response.count if response.count else 0
    except:
        return 0

def get_average_overall_score():
    """í‰ê·  ì¢…í•©ì ìˆ˜ ì¡°íšŒ - JSON í•„ë“œì—ì„œ ì¶”ì¶œ"""
    try:
        client = simple_client.get_client()
        if not client:
            return 0
        
        response = client.table("ai_influencer_analyses").select("evaluation").execute()
        scores = []
        
        for item in response.data:
            evaluation = item.get("evaluation", {})
            if isinstance(evaluation, dict):
                overall_score = evaluation.get("overall_score")
                if overall_score is not None:
                    try:
                        scores.append(float(overall_score))
                    except (ValueError, TypeError):
                        pass
        
        return sum(scores) / len(scores) if scores else 0
    except:
        return 0

def get_recommendation_distribution():
    """ì¶”ì²œë„ ë¶„í¬ ì¡°íšŒ"""
    try:
        client = simple_client.get_client()
        if not client:
            return {}
        
        response = client.table("ai_influencer_analyses").select("recommendation").execute()
        recommendations = [item["recommendation"] for item in response.data if item.get("recommendation")]
        
        distribution = {}
        for rec in recommendations:
            distribution[rec] = distribution.get(rec, 0) + 1
        
        return distribution
    except:
        return {}

def get_category_statistics():
    """ì¹´í…Œê³ ë¦¬ë³„ í†µê³„ ì¡°íšŒ"""
    try:
        client = simple_client.get_client()
        if not client:
            return {}
        
        response = client.table("ai_influencer_analyses").select("category").execute()
        categories = [item["category"] for item in response.data if item.get("category")]
        
        stats = {}
        for category in categories:
            stats[category] = stats.get(category, 0) + 1
        
        return stats
    except:
        return {}

def get_score_distribution():
    """ì ìˆ˜ ë¶„í¬ ì¡°íšŒ"""
    try:
        client = simple_client.get_client()
        if not client:
            return []
        
        response = client.table("ai_influencer_analyses").select("overall_score").not_.is_("overall_score", "null").execute()
        return [item["overall_score"] for item in response.data if item.get("overall_score")]
    except:
        return []

def get_basic_account_statistics():
    """ê¸°ë³¸ ê³„ì • ì •ë³´ í†µê³„ ì¡°íšŒ"""
    try:
        client = simple_client.get_client()
        if not client:
            return None
        
        response = client.table("ai_influencer_analyses").select("followers, followings, posts_count").execute()
        
        if not response.data:
            return None
        
        # ë°ì´í„° ì¶”ì¶œ
        followers = [item["followers"] for item in response.data if item.get("followers") is not None]
        followings = [item["followings"] for item in response.data if item.get("followings") is not None]
        posts = [item["posts_count"] for item in response.data if item.get("posts_count") is not None]
        
        # íŒ”ë¡œì›Œ/íŒ”ë¡œì‰ ë¹„ìœ¨ ê³„ì‚°
        ratios = []
        for i in range(min(len(followers), len(followings))):
            if followings[i] and followings[i] > 0:
                ratios.append(followers[i] / followings[i])
        
        return {
            "avg_followers": sum(followers) / len(followers) if followers else 0,
            "median_followers": sorted(followers)[len(followers)//2] if followers else 0,
            "max_followers": max(followers) if followers else 0,
            "min_followers": min(followers) if followers else 0,
            "followers_distribution": followers,
            
            "avg_followings": sum(followings) / len(followings) if followings else 0,
            "median_followings": sorted(followings)[len(followings)//2] if followings else 0,
            "max_followings": max(followings) if followings else 0,
            "min_followings": min(followings) if followings else 0,
            
            "avg_posts": sum(posts) / len(posts) if posts else 0,
            "median_posts": sorted(posts)[len(posts)//2] if posts else 0,
            "max_posts": max(posts) if posts else 0,
            "min_posts": min(posts) if posts else 0,
            
            "avg_ratio": sum(ratios) / len(ratios) if ratios else 0,
            "median_ratio": sorted(ratios)[len(ratios)//2] if ratios else 0,
            "max_ratio": max(ratios) if ratios else 0,
            "min_ratio": min(ratios) if ratios else 0,
            "ratio_distribution": ratios
        }
    except Exception as e:
        st.error(f"ê¸°ë³¸ ê³„ì • ì •ë³´ í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None

def get_network_analysis_statistics():
    """ë„¤íŠ¸ì›Œí¬ ë¶„ì„ í†µê³„ ì¡°íšŒ"""
    try:
        client = simple_client.get_client()
        if not client:
            return None
        
        response = client.table("ai_influencer_analyses").select("follow_network_analysis").execute()
        
        if not response.data:
            return None
        
        authenticity_scores = []
        network_types = []
        
        for item in response.data:
            network_analysis = item.get("follow_network_analysis", {})
            if isinstance(network_analysis, dict):
                # ì˜í–¥ë ¥ ì§„ì •ì„± ì ìˆ˜ ì¶”ì¶œ
                score = network_analysis.get("influence_authenticity_score")
                if score is not None:
                    try:
                        authenticity_scores.append(float(score))
                    except (ValueError, TypeError):
                        pass
                
                # ë„¤íŠ¸ì›Œí¬ ìœ í˜• ì¶”ì¶œ
                network_type = network_analysis.get("network_type_inference")
                if network_type:
                    network_types.append(network_type)
        
        # ë„¤íŠ¸ì›Œí¬ ìœ í˜• ë¶„í¬ ê³„ì‚°
        network_type_dist = {}
        for nt in network_types:
            network_type_dist[nt] = network_type_dist.get(nt, 0) + 1
        
        return {
            "avg_authenticity_score": sum(authenticity_scores) / len(authenticity_scores) if authenticity_scores else 0,
            "median_authenticity_score": sorted(authenticity_scores)[len(authenticity_scores)//2] if authenticity_scores else 0,
            "max_authenticity_score": max(authenticity_scores) if authenticity_scores else 0,
            "min_authenticity_score": min(authenticity_scores) if authenticity_scores else 0,
            "authenticity_distribution": authenticity_scores,
            "network_type_distribution": network_type_dist
        }
    except Exception as e:
        st.error(f"ë„¤íŠ¸ì›Œí¬ ë¶„ì„ í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None

def get_activity_metrics_statistics():
    """í™œë™ì„± ë©”íŠ¸ë¦­ í†µê³„ ì¡°íšŒ"""
    try:
        client = simple_client.get_client()
        if not client:
            return None
        
        response = client.table("ai_influencer_analyses").select("follow_network_analysis, comment_authenticity_analysis").execute()
        
        if not response.data:
            return None
        
        likes = []
        comments = []
        engagement_rates = []
        recency_spans = []
        posting_paces = []
        
        for item in response.data:
            network_analysis = item.get("follow_network_analysis", {})
            comment_analysis = item.get("comment_authenticity_analysis", {})
            
            if isinstance(network_analysis, dict):
                # ìµœê·¼ 5ê°œ í¬ìŠ¤íŠ¸ ë©”íŠ¸ë¦­ ì¶”ì¶œ
                avg_likes = network_analysis.get("avg_likes_last5")
                if avg_likes is not None:
                    try:
                        likes.append(float(avg_likes))
                    except (ValueError, TypeError):
                        pass
                
                # í™œë™ ì£¼ê¸° ì¶”ì¶œ
                recency_span = network_analysis.get("recency_span_last5_days")
                if recency_span is not None:
                    try:
                        recency_spans.append(float(recency_span))
                    except (ValueError, TypeError):
                        pass
                
                # ê²Œì‹œ ë¹ˆë„ ì¶”ì¶œ
                posting_pace = network_analysis.get("posting_pace_last5")
                if posting_pace:
                    posting_paces.append(posting_pace)
                
                # ì°¸ì—¬ìœ¨ ì¶”ì¶œ
                engagement_rate = network_analysis.get("est_engagement_rate_last5")
                if engagement_rate is not None:
                    try:
                        engagement_rates.append(float(engagement_rate))
                    except (ValueError, TypeError):
                        pass
            
            if isinstance(comment_analysis, dict):
                # í‰ê·  ëŒ“ê¸€ ìˆ˜ ì¶”ì¶œ
                avg_comments = comment_analysis.get("comments_avg_last5")
                if avg_comments is not None:
                    try:
                        comments.append(float(avg_comments))
                    except (ValueError, TypeError):
                        pass
        
        # ê²Œì‹œ ë¹ˆë„ ë¶„í¬ ê³„ì‚°
        posting_pace_dist = {}
        for pp in posting_paces:
            posting_pace_dist[pp] = posting_pace_dist.get(pp, 0) + 1
        
        return {
            "avg_likes": sum(likes) / len(likes) if likes else 0,
            "median_likes": sorted(likes)[len(likes)//2] if likes else 0,
            "avg_comments": sum(comments) / len(comments) if comments else 0,
            "avg_engagement_rate": sum(engagement_rates) / len(engagement_rates) if engagement_rates else 0,
            "avg_recency_span": sum(recency_spans) / len(recency_spans) if recency_spans else 0,
            "median_recency_span": sorted(recency_spans)[len(recency_spans)//2] if recency_spans else 0,
            "min_recency_span": min(recency_spans) if recency_spans else 0,
            "max_recency_span": max(recency_spans) if recency_spans else 0,
            "posting_pace_distribution": posting_pace_dist,
            "engagement_rate_distribution": engagement_rates
        }
    except Exception as e:
        st.error(f"í™œë™ì„± ë©”íŠ¸ë¦­ í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None

def get_comment_authenticity_statistics():
    """ëŒ“ê¸€ ì§„ì •ì„± ë¶„ì„ í†µê³„ ì¡°íšŒ"""
    try:
        client = simple_client.get_client()
        if not client:
            return None
        
        response = client.table("ai_influencer_analyses").select("comment_authenticity_analysis").execute()
        
        if not response.data:
            return None
        
        authentic_ratios = []
        low_authentic_ratios = []
        authenticity_levels = []
        
        for item in response.data:
            comment_analysis = item.get("comment_authenticity_analysis", {})
            if isinstance(comment_analysis, dict):
                # ì§„ì •ì„± ë¹„ìœ¨ ì¶”ì¶œ - ratio_estimation ë‚´ë¶€ì—ì„œ ì¶”ì¶œ
                ratio_estimation = comment_analysis.get("ratio_estimation", {})
                if isinstance(ratio_estimation, dict):
                    # authentic_comments_ratio ì¶”ì¶œ (ë¬¸ìì—´ì—ì„œ ìˆ«ì ì¶”ì¶œ)
                    authentic_ratio_str = ratio_estimation.get("authentic_comments_ratio", "")
                    if authentic_ratio_str:
                        try:
                            # "ì•½ 40%" í˜•íƒœì—ì„œ ìˆ«ìë§Œ ì¶”ì¶œ
                            import re
                            match = re.search(r'(\d+(?:\.\d+)?)', str(authentic_ratio_str))
                            if match:
                                authentic_ratios.append(float(match.group(1)))
                        except (ValueError, TypeError):
                            pass
                    
                    # low_authentic_comments_ratio ì¶”ì¶œ (ë¬¸ìì—´ì—ì„œ ìˆ«ì ì¶”ì¶œ)
                    low_authentic_ratio_str = ratio_estimation.get("low_authentic_comments_ratio", "")
                    if low_authentic_ratio_str:
                        try:
                            # "ì•½ 60%" í˜•íƒœì—ì„œ ìˆ«ìë§Œ ì¶”ì¶œ
                            import re
                            match = re.search(r'(\d+(?:\.\d+)?)', str(low_authentic_ratio_str))
                            if match:
                                low_authentic_ratios.append(float(match.group(1)))
                        except (ValueError, TypeError):
                            pass
                
                # ì§„ì •ì„± ë“±ê¸‰ ì¶”ì¶œ
                authenticity_level = comment_analysis.get("authenticity_level")
                if authenticity_level:
                    authenticity_levels.append(authenticity_level)
        
        # ì§„ì •ì„± ë“±ê¸‰ ë¶„í¬ ê³„ì‚°
        authenticity_level_dist = {}
        for al in authenticity_levels:
            authenticity_level_dist[al] = authenticity_level_dist.get(al, 0) + 1
        
        return {
            "avg_authentic_ratio": sum(authentic_ratios) / len(authentic_ratios) if authentic_ratios else 0,
            "median_authentic_ratio": sorted(authentic_ratios)[len(authentic_ratios)//2] if authentic_ratios else 0,
            "avg_low_authentic_ratio": sum(low_authentic_ratios) / len(low_authentic_ratios) if low_authentic_ratios else 0,
            "median_low_authentic_ratio": sorted(low_authentic_ratios)[len(low_authentic_ratios)//2] if low_authentic_ratios else 0,
            "authentic_ratio_distribution": authentic_ratios,
            "low_authentic_ratio_distribution": low_authentic_ratios,
            "authenticity_level_distribution": authenticity_level_dist
        }
    except Exception as e:
        st.error(f"ëŒ“ê¸€ ì§„ì •ì„± í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None

def render_top_bottom_distribution_plot():
    """ìƒìœ„/í•˜ìœ„ ì¸í”Œë£¨ì–¸ì„œ ë¶„í¬ë„ ë Œë”ë§"""
    try:
        # ìƒìœ„/í•˜ìœ„ ì¸í”Œë£¨ì–¸ì„œ ë°ì´í„° ì¡°íšŒ
        top_bottom_analysis = get_top_bottom_authenticity_analysis()
        
        if not top_bottom_analysis or not top_bottom_analysis['top_influencers'] or not top_bottom_analysis['bottom_influencers']:
            st.info("ë¶„í¬ë„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ë°ì´í„° ì¤€ë¹„
        plot_data = []
        
        # ìƒìœ„ ì¸í”Œë£¨ì–¸ì„œ ë°ì´í„° ì¶”ê°€
        for inf in top_bottom_analysis['top_influencers']:
            plot_data.append({
                'name': inf['name'],
                'influencer_id': inf['influencer_id'],
                'authentic_ratio': inf['authentic_ratio'],
                'low_authentic_ratio': inf['low_authentic_ratio'],
                'authenticity_level': inf['authenticity_level'],
                'group': 'ìƒìœ„ (í‰ê·  ì´ìƒ)',
                'size': 8
            })
        
        # í•˜ìœ„ ì¸í”Œë£¨ì–¸ì„œ ë°ì´í„° ì¶”ê°€
        for inf in top_bottom_analysis['bottom_influencers']:
            plot_data.append({
                'name': inf['name'],
                'influencer_id': inf['influencer_id'],
                'authentic_ratio': inf['authentic_ratio'],
                'low_authentic_ratio': inf['low_authentic_ratio'],
                'authenticity_level': inf['authenticity_level'],
                'group': 'í•˜ìœ„ (í‰ê·  ë¯¸ë§Œ)',
                'size': 6
            })
        
        if not plot_data:
            st.info("ë¶„í¬ë„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ë¶„í¬ë„ ìƒì„±
        fig = px.scatter(
            plot_data,
            x='authentic_ratio',
            y='low_authentic_ratio',
            color='authenticity_level',
            symbol='group',
            size='size',
            hover_data=['name', 'influencer_id'],
            title="ìƒìœ„/í•˜ìœ„ ì¸í”Œë£¨ì–¸ì„œ ë¶„í¬ë„",
            labels={
                'authentic_ratio': 'ì§„ì •ì„± ë¹„ìœ¨ (%)',
                'low_authentic_ratio': 'í˜•ì‹ì  ëŒ“ê¸€ ë¹„ìœ¨ (%)',
                'authenticity_level': 'AI ë¶„ì„ ë“±ê¸‰',
                'group': 'ê·¸ë£¹'
            },
            color_discrete_map={
                'ë†’ìŒ': '#2E8B57',
                'ì¤‘ê°„': '#FFD700',
                'ë‚®ìŒ': '#DC143C'
            },
            symbol_map={
                'ìƒìœ„ (í‰ê·  ì´ìƒ)': 'circle',
                'í•˜ìœ„ (í‰ê·  ë¯¸ë§Œ)': 'diamond'
            }
        )
        
        # í‰ê· ì„  ì¶”ê°€
        avg_authentic = top_bottom_analysis['avg_authentic_ratio']
        fig.add_vline(
            x=avg_authentic,
            line_dash="dash",
            line_color="red",
            annotation_text=f"í‰ê·  ì§„ì •ì„±: {avg_authentic:.1f}%",
            annotation_position="top"
        )
        
        # ë ˆì´ì•„ì›ƒ ì¡°ì •
        fig.update_layout(
            width=500,
            height=400,
            showlegend=True,
            legend=dict(
                orientation="v",
                yanchor="top",
                y=1,
                xanchor="left",
                x=1.02
            )
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        # ë²”ë¡€ ì„¤ëª…
        st.caption("""
        **ë²”ë¡€ ì„¤ëª…:**
        - ğŸŸ¢ ë†’ìŒ: ì‹¤ì œ íŒ¬ë“¤ì˜ ì§„ì •í•œ ë°˜ì‘ì´ ë§ìŒ
        - ğŸŸ¡ ì¤‘ê°„: ì¼ë¶€ ì§„ì •í•œ ë°˜ì‘ê³¼ í˜•ì‹ì  ëŒ“ê¸€ í˜¼ì¬  
        - ğŸ”´ ë‚®ìŒ: ëŒ€ë¶€ë¶„ í˜•ì‹ì ì´ê±°ë‚˜ ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ëŒ“ê¸€
        - âšª ìƒìœ„: í‰ê·  ì´ìƒ ì§„ì •ì„±
        - ğŸ”· í•˜ìœ„: í‰ê·  ë¯¸ë§Œ ì§„ì •ì„±
        """)
        
    except Exception as e:
        st.error(f"ë¶„í¬ë„ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")

def get_top_bottom_authenticity_analysis():
    """í‰ê·  ëŒ€ë¹„ ìƒìœ„/í•˜ìœ„ ì¸í”Œë£¨ì–¸ì„œ ë¶„ì„ ì¡°íšŒ"""
    try:
        client = simple_client.get_client()
        if not client:
            return None
        
        # ì „ì²´ ë°ì´í„° ì¡°íšŒ
        response = client.table("ai_influencer_analyses").select("name, influencer_id, comment_authenticity_analysis").execute()
        
        if not response.data:
            return None
        
        # ì§„ì •ì„± ë¹„ìœ¨ ì¶”ì¶œ ë° í‰ê·  ê³„ì‚°
        influencers_data = []
        authentic_ratios = []
        
        for item in response.data:
            comment_analysis = item.get("comment_authenticity_analysis", {})
            if isinstance(comment_analysis, dict):
                ratio_estimation = comment_analysis.get("ratio_estimation", {})
                if isinstance(ratio_estimation, dict):
                    # ì§„ì •ì„± ë¹„ìœ¨ ì¶”ì¶œ
                    authentic_ratio_str = ratio_estimation.get("authentic_comments_ratio", "")
                    low_authentic_ratio_str = ratio_estimation.get("low_authentic_comments_ratio", "")
                    
                    if authentic_ratio_str and low_authentic_ratio_str:
                        try:
                            import re
                            # "ì•½ 40%" í˜•íƒœì—ì„œ ìˆ«ìë§Œ ì¶”ì¶œ
                            authentic_match = re.search(r'(\d+(?:\.\d+)?)', str(authentic_ratio_str))
                            low_authentic_match = re.search(r'(\d+(?:\.\d+)?)', str(low_authentic_ratio_str))
                            
                            if authentic_match and low_authentic_match:
                                authentic_ratio = float(authentic_match.group(1))
                                low_authentic_ratio = float(low_authentic_match.group(1))
                                
                                influencers_data.append({
                                    "name": item.get("name", "N/A"),
                                    "influencer_id": item.get("influencer_id", "N/A"),
                                    "authentic_ratio": authentic_ratio,
                                    "low_authentic_ratio": low_authentic_ratio,
                                    "authenticity_level": comment_analysis.get("authenticity_level", "N/A")
                                })
                                authentic_ratios.append(authentic_ratio)
                        except (ValueError, TypeError):
                            pass
        
        if not authentic_ratios:
            return None
        
        # í‰ê·  ê³„ì‚°
        avg_authentic_ratio = sum(authentic_ratios) / len(authentic_ratios)
        
        # ìƒìœ„/í•˜ìœ„ ì¸í”Œë£¨ì–¸ì„œ ë¶„ë¥˜
        top_influencers = [inf for inf in influencers_data if inf['authentic_ratio'] >= avg_authentic_ratio]
        bottom_influencers = [inf for inf in influencers_data if inf['authentic_ratio'] < avg_authentic_ratio]
        
        # ì§„ì •ì„± ë¹„ìœ¨ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        top_influencers.sort(key=lambda x: x['authentic_ratio'], reverse=True)
        bottom_influencers.sort(key=lambda x: x['authentic_ratio'], reverse=True)
        
        return {
            "top_influencers": top_influencers,
            "bottom_influencers": bottom_influencers,
            "avg_authentic_ratio": avg_authentic_ratio,
            "total_count": len(influencers_data)
        }
        
    except Exception as e:
        st.error(f"ìƒìœ„/í•˜ìœ„ ì¸í”Œë£¨ì–¸ì„œ ë¶„ì„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None

def get_evaluation_scores_statistics():
    """í‰ê°€ ì ìˆ˜ í†µê³„ ì¡°íšŒ"""
    try:
        client = simple_client.get_client()
        if not client:
            return None
        
        response = client.table("ai_influencer_analyses").select("evaluation, content_analysis").execute()
        
        if not response.data:
            return None
        
        # ê° ì ìˆ˜ë³„ ë°ì´í„° ì¶”ì¶œ (JSON í•„ë“œì—ì„œ)
        engagement_scores = []
        activity_scores = []
        communication_scores = []
        growth_potential_scores = []
        overall_scores = []
        
        for item in response.data:
            evaluation = item.get("evaluation", {})
            if isinstance(evaluation, dict):
                # engagement ì ìˆ˜ ì¶”ì¶œ
                engagement = evaluation.get("engagement")
                if engagement is not None:
                    try:
                        engagement_scores.append(float(engagement))
                    except (ValueError, TypeError):
                        pass
                
                # activity ì ìˆ˜ ì¶”ì¶œ
                activity = evaluation.get("activity")
                if activity is not None:
                    try:
                        activity_scores.append(float(activity))
                    except (ValueError, TypeError):
                        pass
                
                # communication ì ìˆ˜ ì¶”ì¶œ
                communication = evaluation.get("communication")
                if communication is not None:
                    try:
                        communication_scores.append(float(communication))
                    except (ValueError, TypeError):
                        pass
                
                # growth_potential ì ìˆ˜ ì¶”ì¶œ
                growth_potential = evaluation.get("growth_potential")
                if growth_potential is not None:
                    try:
                        growth_potential_scores.append(float(growth_potential))
                    except (ValueError, TypeError):
                        pass
                
                # overall_score ì ìˆ˜ ì¶”ì¶œ
                overall_score = evaluation.get("overall_score")
                if overall_score is not None:
                    try:
                        overall_scores.append(float(overall_score))
                    except (ValueError, TypeError):
                        pass
        
        # inference_confidenceëŠ” content_analysis JSONì—ì„œ ì¶”ì¶œ
        inference_confidences = []
        for item in response.data:
            content_analysis = item.get("content_analysis", {})
            if isinstance(content_analysis, dict):
                confidence = content_analysis.get("inference_confidence")
                if confidence is not None:
                    try:
                        inference_confidences.append(float(confidence))
                    except (ValueError, TypeError):
                        pass
        
        # ìƒê´€ê´€ê³„ ë°ì´í„° ì¤€ë¹„
        import pandas as pd
        correlation_data = None
        if len(engagement_scores) > 1:
            df = pd.DataFrame({
                'engagement': engagement_scores[:len(engagement_scores)],
                'activity': activity_scores[:len(engagement_scores)],
                'communication': communication_scores[:len(engagement_scores)],
                'growth_potential': growth_potential_scores[:len(engagement_scores)],
                'overall': overall_scores[:len(engagement_scores)]
            })
            correlation_data = df.corr().values.tolist()
        
        return {
            "avg_engagement": sum(engagement_scores) / len(engagement_scores) if engagement_scores else 0,
            "median_engagement": sorted(engagement_scores)[len(engagement_scores)//2] if engagement_scores else 0,
            "max_engagement": max(engagement_scores) if engagement_scores else 0,
            "min_engagement": min(engagement_scores) if engagement_scores else 0,
            
            "avg_activity": sum(activity_scores) / len(activity_scores) if activity_scores else 0,
            "median_activity": sorted(activity_scores)[len(activity_scores)//2] if activity_scores else 0,
            "max_activity": max(activity_scores) if activity_scores else 0,
            "min_activity": min(activity_scores) if activity_scores else 0,
            
            "avg_communication": sum(communication_scores) / len(communication_scores) if communication_scores else 0,
            "median_communication": sorted(communication_scores)[len(communication_scores)//2] if communication_scores else 0,
            "max_communication": max(communication_scores) if communication_scores else 0,
            "min_communication": min(communication_scores) if communication_scores else 0,
            
            "avg_growth_potential": sum(growth_potential_scores) / len(growth_potential_scores) if growth_potential_scores else 0,
            "median_growth_potential": sorted(growth_potential_scores)[len(growth_potential_scores)//2] if growth_potential_scores else 0,
            "max_growth_potential": max(growth_potential_scores) if growth_potential_scores else 0,
            "min_growth_potential": min(growth_potential_scores) if growth_potential_scores else 0,
            
            "avg_overall": sum(overall_scores) / len(overall_scores) if overall_scores else 0,
            "median_overall": sorted(overall_scores)[len(overall_scores)//2] if overall_scores else 0,
            "max_overall": max(overall_scores) if overall_scores else 0,
            "min_overall": min(overall_scores) if overall_scores else 0,
            
            "score_distributions": {
                "engagement": engagement_scores,
                "activity": activity_scores,
                "communication": communication_scores,
                "growth_potential": growth_potential_scores,
                "overall": overall_scores
            },
            "inference_confidence_distribution": inference_confidences,
            "correlation_data": correlation_data
        }
    except Exception as e:
        st.error(f"í‰ê°€ ì ìˆ˜ í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None
