"""
AI ë¶„ì„ ê³µí†µ í•¨ìˆ˜ë“¤
"""
import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from typing import Dict, Any, List, Optional
from datetime import datetime, timedelta
import json
import os
import numpy as np
import time
from ..db.database import db_manager
from ..supabase.simple_client import simple_client

def get_completed_crawling_data(client, limit=1000, offset=0):
    """í¬ë¡¤ë§ ì™„ë£Œë˜ê³  AI ë¶„ì„ì´ í•„ìš”í•œ ë°ì´í„° ì¡°íšŒ (í˜ì´ì§•) - ì¬ì‹œë„ í¬í•¨"""
    max_retries = 3
    retry_delay = 1
    for attempt in range(max_retries):
        try:
            if not client:
                return []
            
            # ai_analysis_status í…Œì´ë¸”ì—ì„œ is_analyzed = FALSEì¸ IDë“¤ì„ ë¨¼ì € ì¡°íšŒ
            analysis_status_response = client.table("ai_analysis_status").select("id")\
                .eq("is_analyzed", False)\
                .range(offset, offset + limit - 1).execute()
            
            if not analysis_status_response.data:
                return []
            
            # ì¡°íšŒëœ IDë“¤ë¡œ tb_instagram_crawling í…Œì´ë¸”ì—ì„œ COMPLETE ìƒíƒœì¸ ë°ì´í„° ì¡°íšŒ
            crawling_ids = [item["id"] for item in analysis_status_response.data]
            response = client.table("tb_instagram_crawling").select("*")\
                .in_("id", crawling_ids)\
                .eq("status", "COMPLETE").execute()
            
            return response.data if response.data else []
        except Exception as e:
            error_msg = str(e)
            if "Server disconnected" in error_msg or "connection" in error_msg.lower():
                if attempt < max_retries - 1:
                    st.warning(f"ì„œë²„ ì—°ê²° ì˜¤ë¥˜. {retry_delay}s í›„ ì¬ì‹œë„... ({attempt + 1}/{max_retries})")
                    time.sleep(retry_delay); retry_delay *= 2; continue
                else:
                    st.error(f"í¬ë¡¤ë§ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨(ì¬ì‹œë„ ì´ˆê³¼): {error_msg}")
                    return []
            else:
                st.error(f"í¬ë¡¤ë§ ë°ì´í„° ì¡°íšŒ ì˜¤ë¥˜: {error_msg}")
                return []
    return []

def get_completed_crawling_data_count(client):
    """í¬ë¡¤ë§ ì™„ë£Œë˜ê³  AI ë¶„ì„ì´ í•„ìš”í•œ ë°ì´í„° ì´ ê°œìˆ˜"""
    max_retries = 3
    retry_delay = 1
    for attempt in range(max_retries):
        try:
            if not client:
                return 0
            
            # ai_analysis_status í…Œì´ë¸”ì—ì„œ is_analyzed = FALSEì¸ í•­ëª©ì˜ countë¥¼ ë¨¼ì € ê°€ì ¸ì˜¤ê¸°
            # count="exact"ë¥¼ ì‚¬ìš©í•˜ì—¬ limit ë¬¸ì œë¥¼ í”¼í•¨
            analysis_status_count_response = client.table("ai_analysis_status").select("id", count="exact")\
                .eq("is_analyzed", False).execute()
            
            if not analysis_status_count_response.count or analysis_status_count_response.count == 0:
                return 0
            
            # ai_analysis_status í…Œì´ë¸”ì—ì„œ is_analyzed = FALSEì¸ IDë“¤ì„ ë°°ì¹˜ë¡œ ì¡°íšŒ
            # Supabaseì˜ ê¸°ë³¸ limit(1000)ì„ ê³ ë ¤í•˜ì—¬ ë°°ì¹˜ ì²˜ë¦¬
            total_count = 0
            batch_size = 1000
            total_batches = (analysis_status_count_response.count + batch_size - 1) // batch_size
            
            for batch_num in range(total_batches):
                offset = batch_num * batch_size
                analysis_status_response = client.table("ai_analysis_status").select("id")\
                    .eq("is_analyzed", False)\
                    .range(offset, offset + batch_size - 1).execute()
                
                if not analysis_status_response.data:
                    break
                
                # ì¡°íšŒëœ IDë“¤ë¡œ tb_instagram_crawling í…Œì´ë¸”ì—ì„œ COMPLETE ìƒíƒœì¸ ë°ì´í„° ê°œìˆ˜ ì¡°íšŒ
                crawling_ids = [item["id"] for item in analysis_status_response.data]
                response = client.table("tb_instagram_crawling").select("id", count="exact")\
                    .in_("id", crawling_ids)\
                    .eq("status", "COMPLETE").execute()
                
                total_count += response.count if response.count else 0
            
            return total_count
        except Exception as e:
            error_msg = str(e)
            if "Server disconnected" in error_msg or "connection" in error_msg.lower():
                if attempt < max_retries - 1:
                    st.warning(f"ì„œë²„ ì—°ê²° ì˜¤ë¥˜. {retry_delay}s í›„ ì¬ì‹œë„... ({attempt + 1}/{max_retries})")
                    time.sleep(retry_delay); retry_delay *= 2; continue
                else:
                    st.error(f"ê°œìˆ˜ ì¡°íšŒ ì‹¤íŒ¨(ì¬ì‹œë„ ì´ˆê³¼): {error_msg}")
                    return 0
            else:
                st.error(f"ê°œìˆ˜ ì¡°íšŒ ì˜¤ë¥˜: {error_msg}")
                return 0
    return 0

def is_recently_analyzed_by_id(client, crawling_id):
    """í¬ë¡¤ë§ ID ìµœê·¼ ë¶„ì„ ì—¬ë¶€(30ì¼) - AI ë¶„ì„ ìƒíƒœ í…Œì´ë¸” ì‚¬ìš©"""
    max_retries = 3
    retry_delay = 1
    for attempt in range(max_retries):
        try:
            if not client:
                return False
            one_month_ago = datetime.now() - timedelta(days=30)
            # ai_analysis_status í…Œì´ë¸”ì—ì„œ í™•ì¸
            response = client.table("ai_analysis_status").select("is_analyzed", "analyzed_at")\
                .eq("id", crawling_id)\
                .eq("is_analyzed", True)\
                .gte("analyzed_at", one_month_ago.isoformat()).execute()
            return bool(response.data)
        except Exception as e:
            error_msg = str(e)
            if "Server disconnected" in error_msg or "connection" in error_msg.lower():
                if attempt < max_retries - 1:
                    st.warning(f"ì„œë²„ ì—°ê²° ì˜¤ë¥˜. {retry_delay}s í›„ ì¬ì‹œë„... ({attempt + 1}/{max_retries})")
                    time.sleep(retry_delay); retry_delay *= 2; continue
                else:
                    st.error(f"ìµœê·¼ ë¶„ì„ ì—¬ë¶€ í™•ì¸ ì‹¤íŒ¨(ì¬ì‹œë„ ì´ˆê³¼): {error_msg}")
                    return False
            else:
                st.error(f"ìµœê·¼ ë¶„ì„ ì—¬ë¶€ í™•ì¸ ì˜¤ë¥˜: {error_msg}")
                return False
    return False

def save_ai_analysis_result(client, crawling_data, analysis_result, crawling_id):
    """AI ë¶„ì„ ê²°ê³¼ ì €ì¥ - client ì£¼ì… ë²„ì „"""
    max_retries = 3
    retry_delay = 1
    for attempt in range(max_retries):
        try:
            if not client:
                raise Exception("Supabase í´ë¼ì´ì–¸íŠ¸ ì—†ìŒ")

            # ì¶”ì ìš© crawling_id ì£¼ì…
            if "notes" in analysis_result and isinstance(analysis_result["notes"], dict):
                analysis_result["notes"]["crawling_id"] = crawling_id

            # ìƒˆ í…Œì´ë¸” ì‚¬ìš© (ai_influencer_analyses_new)
            # unique constraint: (influencer_id, alias, analyzed_on) ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì²´í¬
            analyzed_on = analysis_result.get("analyzed_on", datetime.now().date().isoformat())
            alias = analysis_result.get("alias", "")
            
            existing_response = client.table("ai_influencer_analyses_new").select("id")\
                .eq("influencer_id", crawling_id)\
                .eq("alias", alias)\
                .eq("analyzed_on", analyzed_on)\
                .execute()

            if existing_response.data:
                client.table("ai_influencer_analyses_new").update(analysis_result)\
                    .eq("id", existing_response.data[0]["id"]).execute()
            else:
                client.table("ai_influencer_analyses_new").insert(analysis_result).execute()
            
            # AI ë¶„ì„ ìƒíƒœ ì—…ë°ì´íŠ¸ (íŠ¸ë¦¬ê±°ê°€ ìˆì§€ë§Œ ëª…ì‹œì ìœ¼ë¡œë„ ì—…ë°ì´íŠ¸)
            try:
                analyzed_at = analysis_result.get("analyzed_at", datetime.now().isoformat())
                client.table("ai_analysis_status").upsert({
                    "id": crawling_id,
                    "is_analyzed": True,
                    "analyzed_at": analyzed_at,
                    "updated_at": datetime.now().isoformat()
                }).execute()
                
                # tb_instagram_crawling í…Œì´ë¸”ì˜ AI ë¶„ì„ ìƒíƒœë„ ì—…ë°ì´íŠ¸
                client.table("tb_instagram_crawling").update({
                    "ai_analysis_status": True,
                    "ai_analyzed_at": analyzed_at,
                    "updated_at": datetime.now().isoformat()
                }).eq("id", crawling_id).execute()
            except Exception as status_error:
                st.warning(f"AI ë¶„ì„ ìƒíƒœ ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜ (ë¶„ì„ ê²°ê³¼ëŠ” ì €ì¥ë¨): {str(status_error)}")
            
            return

        except Exception as e:
            error_msg = str(e)
            if "Server disconnected" in error_msg or "connection" in error_msg.lower():
                if attempt < max_retries - 1:
                    st.warning(f"ì„œë²„ ì—°ê²° ì˜¤ë¥˜. {retry_delay}s í›„ ì¬ì‹œë„... ({attempt + 1}/{max_retries})")
                    time.sleep(retry_delay); retry_delay *= 2; continue
                else:
                    st.error(f"AI ë¶„ì„ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨(ì¬ì‹œë„ ì´ˆê³¼): {error_msg}")
                    raise
            else:
                st.error(f"AI ë¶„ì„ ê²°ê³¼ ì €ì¥ ì˜¤ë¥˜: {error_msg}")
                raise

def perform_ai_analysis(data):
    """AI ë¶„ì„ ìˆ˜í–‰ - 5ë¶„ íƒ€ì„ì•„ì›ƒ, ì¬ì‹œë„ ì—†ìŒ"""
    from openai import OpenAI
    import time

    timeout_seconds = 300  # 5ë¶„ íƒ€ì„ì•„ì›ƒ

    # API í‚¤ ì½ê¸° (í™˜ê²½ë³€ìˆ˜ ìš°ì„ , ê·¸ ë‹¤ìŒ secrets)
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        try:
            api_key = st.secrets["OPENAI_API_KEY"]
        except (KeyError, AttributeError):
            api_key = None
    
    if not api_key or api_key == "your-openai-api-key-here":
        st.error("í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return None

    client = OpenAI(api_key=api_key)

    # ëª¨ë¸ ëª…ì‹œ í•„ìˆ˜ (secretsì—ì„œ ì˜¤ë²„ë¼ì´ë“œ ê°€ëŠ¥)
    model = st.secrets.get("OPENAI_MODEL", "gpt-5-mini")
    prompt_id = st.secrets.get("OPENAI_PROMPT_ID", "pmpt_68f36e44eab08196b4e75067a3074b7b0c099d8443a9dd49")
    prompt_version = st.secrets.get("OPENAI_PROMPT_VERSION")
    prompt_payload = {"id": prompt_id}
    if prompt_version:
        prompt_payload["version"] = prompt_version
    else:
        st.warning("í”„ë¡¬í”„íŠ¸ ë²„ì „ì´ ì—†ì–´ ìµœì‹  ë²„ì „ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")

    input_data = json.dumps(data, ensure_ascii=False)

    try:
        resp = client.responses.create(
            model=model,
            prompt=prompt_payload,
            input=input_data,
            reasoning={"summary": "auto"},
            store=True,
            include=["reasoning.encrypted_content", "web_search_call.action.sources"],
            timeout=timeout_seconds,  # 5ë¶„ timeout
        )
        return parse_ai_response(resp)

    except Exception as e:
        st.error(f"AI ë¶„ì„ ìˆ˜í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        return None

def parse_ai_response(response):
    """Responses API í‘œì¤€ íŒŒì„œ: output_text ìš°ì„ , fallbackë¡œ content[*].text, ì½”ë“œíœìŠ¤ JSON ì¶”ì¶œ"""
    try:
        text = None

        if getattr(response, "output_text", None):
            text = response.output_text
        elif getattr(response, "output", None):
            chunks = []
            for block in (response.output or []):
                for c in getattr(block, "content", []) or []:
                    if hasattr(c, "text") and c.text:
                        chunks.append(c.text)
            text = "\n".join(chunks) if chunks else None

        if not text:
            st.error("ì‘ë‹µì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return None

        import re, json as _json
        # ```json ... ``` ìš°ì„ 
        m = re.search(r"```json\s*(\{.*?\}|\[.*?\])\s*```", text, flags=re.S)
        if m:
            text = m.group(1)

        return _json.loads(text)

    except Exception as e:
        st.error(f"AI ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜: {e}")
        return None

def transform_to_db_format(ai_input_data, ai_result, crawling_id):
    """AI ë¶„ì„ ê²°ê³¼ë¥¼ ai_influencer_analyses_new í…Œì´ë¸” êµ¬ì¡°ì— ë§ê²Œ ë³€í™˜"""
    try:
        # ê¸°ë³¸ ë°ì´í„° ì¶”ì¶œ (AI ë¶„ì„ ê²°ê³¼ì—ì„œ ì¶”ì¶œ)
        # ai_input_dataëŠ” {"id": "", "description": "", "posts": ""} í˜•íƒœ
        influencer_id = crawling_id  # influencer_idëŠ” ì´ì œ VARCHAR íƒ€ì…ì´ë¯€ë¡œ crawling_id ì‚¬ìš©
        platform = "instagram"  # tb_instagram_crawlingì€ ëª¨ë‘ instagram ë°ì´í„°
        
        # AI ë¶„ì„ ê²°ê³¼ì—ì„œ ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ (AIê°€ ë¶„ì„í•œ ê²°ê³¼ ì‚¬ìš©)
        name = ai_result.get("name", "")
        alias = ai_input_data.get("id", "")  # idë¥¼ aliasë¡œ ì‚¬ìš©
        followers = ai_result.get("followers", 0)
        followings = ai_result.get("followings", 0)
        posts_count = ai_result.get("posts_count", 0)
        
        # AI ë¶„ì„ ê²°ê³¼ì—ì„œ ë°ì´í„° ì¶”ì¶œ
        category = ai_result.get("category", "ê¸°íƒ€")
        tags = ai_result.get("tags", [])
        follow_network_analysis = ai_result.get("follow_network_analysis", {})
        comment_authenticity_analysis = ai_result.get("comment_authenticity_analysis", {})
        content_analysis = ai_result.get("content_analysis", {})
        commerce_orientation_analysis = ai_result.get("commerce_orientation_analysis", {})
        evaluation = ai_result.get("evaluation", {})
        insights = ai_result.get("insights", {})
        summary = ai_result.get("summary", "")
        recommendation = ai_result.get("recommendation", "ì¡°ê±´ë¶€")
        notes = ai_result.get("notes", {})
        
        # ë””ë²„ê¹…: AI ì‘ë‹µ êµ¬ì¡° í™•ì¸
        st.write("ğŸ” AI ì‘ë‹µ êµ¬ì¡° í™•ì¸:")
        st.write(f"- name: {name}")
        st.write(f"- category: {category}")
        st.write(f"- tags: {tags}")
        st.write(f"- recommendation: {recommendation}")
        st.write(f"- evaluation keys: {list(evaluation.keys()) if evaluation else 'None'}")
        st.write(f"- content_analysis keys: {list(content_analysis.keys()) if content_analysis else 'None'}")
        
        # ì¶”ì²œë„ ìœ íš¨ì„± ê²€ì¦ ë° ë³€í™˜ (í˜„ì¬ enum ê°’ì— ë§ì¶¤)
        valid_recommendations = ["ì¶”ì²œ", "ì¡°ê±´ë¶€", "ë¹„ì¶”ì²œ"]
        if recommendation not in valid_recommendations:
            # ìœ íš¨í•˜ì§€ ì•Šì€ ê°’ì€ "ì¡°ê±´ë¶€"ë¡œ ê¸°ë³¸ ì„¤ì •
            recommendation = "ì¡°ê±´ë¶€"
        
        # ì ìˆ˜ ìœ íš¨ì„± ê²€ì¦ (0-10 ë²”ìœ„)
        def validate_score(score, default=0):
            try:
                score_val = float(score) if score is not None else default
                return max(0, min(10, score_val))
            except (ValueError, TypeError):
                return default
        
        # evaluation ì ìˆ˜ë“¤ ê²€ì¦
        if isinstance(evaluation, dict):
            evaluation["engagement"] = validate_score(evaluation.get("engagement", 0))
            evaluation["activity"] = validate_score(evaluation.get("activity", 0))
            evaluation["communication"] = validate_score(evaluation.get("communication", 0))
            evaluation["growth_potential"] = validate_score(evaluation.get("growth_potential", 0))
            evaluation["overall_score"] = validate_score(evaluation.get("overall_score", 0))
        
        # inference_confidence ê²€ì¦ (0-1 ë²”ìœ„)
        if isinstance(content_analysis, dict):
            confidence = content_analysis.get("inference_confidence", 0.5)
            try:
                confidence_val = float(confidence) if confidence is not None else 0.5
                content_analysis["inference_confidence"] = max(0, min(1, confidence_val))
            except (ValueError, TypeError):
                content_analysis["inference_confidence"] = 0.5
        
        # notesì— í¬ë¡¤ë§ ID ì¶”ê°€ (ë‚˜ì¤‘ì— save_ai_analysis_resultì—ì„œ ì„¤ì •ë¨)
        if not isinstance(notes, dict):
            notes = {}
        
        # commerce_orientation_analysis ì ìˆ˜ ê²€ì¦ (0-10 ë²”ìœ„)
        if isinstance(commerce_orientation_analysis, dict):
            def validate_commerce_score(score, default=0):
                try:
                    score_val = float(score) if score is not None else default
                    return max(0, min(10, score_val))
                except (ValueError, TypeError):
                    return default
            
            # commerce_orientation_analysis ë‚´ë¶€ ì ìˆ˜ë“¤ ê²€ì¦
            if "monetization_intent_level" in commerce_orientation_analysis:
                commerce_orientation_analysis["monetization_intent_level"] = validate_commerce_score(
                    commerce_orientation_analysis.get("monetization_intent_level", 0)
                )
            if "bragging_orientation_level" in commerce_orientation_analysis:
                commerce_orientation_analysis["bragging_orientation_level"] = validate_commerce_score(
                    commerce_orientation_analysis.get("bragging_orientation_level", 0)
                )
            if "content_fit_for_selling_score" in commerce_orientation_analysis:
                commerce_orientation_analysis["content_fit_for_selling_score"] = validate_commerce_score(
                    commerce_orientation_analysis.get("content_fit_for_selling_score", 0)
                )
        
        # ìµœì¢… ë°ì´í„° êµ¬ì¡° ìƒì„±
        db_data = {
            "influencer_id": influencer_id,
            "platform": platform,
            "name": name,
            "alias": alias,
            "followers": followers,
            "followings": followings,
            "posts_count": posts_count,
            "category": category,
            "tags": tags,
            "follow_network_analysis": follow_network_analysis,
            "comment_authenticity_analysis": comment_authenticity_analysis,
            "content_analysis": content_analysis,
            "commerce_orientation_analysis": commerce_orientation_analysis,
            "evaluation": evaluation,
            "insights": insights,
            "summary": summary,
            "recommendation": recommendation,
            "notes": notes,
            "source": "ai_auto",
            "analyzed_at": datetime.now().isoformat(),
            "analyzed_on": datetime.now().date().isoformat()
        }
        
        # ì ìˆ˜ ê´€ë ¨ ì»¬ëŸ¼ë“¤ì€ ëª¨ë‘ generated columnì´ë¯€ë¡œ ì§ì ‘ ì„¤ì •í•˜ì§€ ì•ŠìŒ
        # evaluation ì ìˆ˜ë“¤ì€ evaluation JSON í•„ë“œì— ì €ì¥ë˜ê³ , 
        # DBì—ì„œ generated columnìœ¼ë¡œ ìë™ ê³„ì‚°ë¨
        # if isinstance(evaluation, dict):
        #     db_data["engagement_score"] = evaluation.get("engagement")
        #     db_data["activity_score"] = evaluation.get("activity")
        #     db_data["communication_score"] = evaluation.get("communication")
        #     db_data["growth_potential_score"] = evaluation.get("growth_potential")
        #     db_data["overall_score"] = evaluation.get("overall_score")

        # inference_confidenceë„ generated columnì´ë¯€ë¡œ ì§ì ‘ ì„¤ì •í•˜ì§€ ì•ŠìŒ
        # if isinstance(content_analysis, dict):
        #     db_data["inference_confidence"] = content_analysis.get("inference_confidence")
        
        return db_data
        
    except Exception as e:
        st.error(f"ë°ì´í„° ë³€í™˜ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None
